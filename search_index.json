[
["index.html", "Solutions to Time Series Analysis with Applications in R Preface", " Solutions to Time Series Analysis with Applications in R Johan Larsson 2017-03-26 Preface This book contains solutions to the problems in the book Time Series Analysis with Applications in R, third edition, by Cryer and Chan. It is provided as a github repository so that anybody may contribute to its development. Unlike the book, the solutions here use lattice graphics when possible instead of base graphics. "],
["introduction.html", "Chapter 1 Introduction 1.1 Larain 1.2 Colors 1.3 Random, normal time series 1.4 Random, \\(\\chi^2\\)-distributed time series 1.5 t(5)-distributed, random values 1.6 Dubuque temperature series", " Chapter 1 Introduction 1.1 Larain Use software to produce the time series plot shown in Exhibit 1.2, on page 2. The data are in the file named larain. library(TSA) library(latticeExtra) data(larain, package = &quot;TSA&quot;) xyplot(larain, ylab = &quot;Inches&quot;, xlab = &quot;Year&quot;, type = &quot;o&quot;) 1.2 Colors Produce the time series plot displayed in Exhibit 1.3, on page 3. The data file is named color. data(color) xyplot(color, ylab = &quot;Color property&quot;, xlab = &quot;Batch&quot;, type = &quot;o&quot;) 1.3 Random, normal time series Simulate a completely random process of length 48 with independent, normal values. Plot the time series plot. Does it look “random”? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rnorm(48))) xyplot(as.ts(rnorm(48))) As far as we can tell there is no discernable pattern here. 1.4 Random, \\(\\chi^2\\)-distributed time series Simulate a completely random process of length 48 with independent, chi-square distributed values, each with 2 degrees of freedom. Display the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rchisq(48, 2))) xyplot(as.ts(rchisq(48, 2))) The process appears random, though non-normal. 1.5 t(5)-distributed, random values Simulate a completely random process of length 48 with independent, t-distributed values each with 5 degrees of freedom. Construct the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rt(48, 5))) xyplot(as.ts(rt(48, 5))) It looks random but not normal, though it should be approximately so, considering the distribution that we have sampled from. 1.6 Dubuque temperature series Construct a time series plot with monthly plotting symbols for the Dubuque temperature series as in Exhibit 1.7, on page 6. The data are in the file named tempdub. data(tempdub) xyplot(tempdub, ylab = &quot;Temperature&quot;, xlab = &quot;Year&quot;) "],
["fundamental-concepts.html", "Chapter 2 Fundamental concepts 2.1 Expected value and covariance 2.2 Covariance and dependence 2.3 Weak stationarity, autocovariance and time plot 2.4 2.5 2.6 2.7 2.8 ", " Chapter 2 Fundamental concepts 2.1 Expected value and covariance Suppose \\(\\text{E}[X) = 2, \\text{Var}[X) = 9, \\text{E}[Y) = 0, \\text{Var}[Y) = 4\\), and \\(\\text{Corr}[X,Y) = 0.25\\). Find: \\(\\text{Var}(X + Y)\\). \\(\\text{Cov}(X, X + Y)\\). \\(\\text{Corr}(X + Y, X − Y)\\). \\begin{align} \\text{Cov}[X,Y] &amp; = \\text{Corr}[X,Y]\\sqrt{Var[X]Var[Y]}\\\\ &amp; = 0.25 \\sqrt{9 \\times 4} = 1.5 \\\\ \\text{Var}[X,Y] &amp; = Var[X]+Var[Y]+2Cov[X,Y]\\\\ &amp; = 9 + 4 + 2 \\times 3 = 16\\\\ \\end{align} \\[\\text{Cov}[X, X+Y] = \\text{Cov}[X,X] + \\text{Cov}[X,Y] = \\text{Var}[X] + \\text{Cov}[X,Y] = 9 + 1.5 = 10.5\\] \\begin{align} \\text{Corr}[X+Y, X-Y] = &amp; \\text{Corr}[X,X] + \\text{Corr}[X,-Y] + \\text{Corr}[Y,X] + \\text{Corr}[Y,-Y] \\\\ = &amp; \\text{Corr}[Y,X] + \\text{Corr}[Y,-Y] \\\\ = &amp; 1 - 0.25 + 0.25 -1 \\\\ = &amp; 0 \\\\ \\end{align} 2.2 Covariance and dependence If \\(X\\) and \\(Y\\) are dependent but \\(\\text{Var}[X) = \\text{Var}[Y)\\), find \\(\\text{Cov}[X + Y, X − Y)\\). \\[ \\text{Cov}[X+Y,X-Y] = \\text{Cov}[X,X] + \\text{Cov}[X,-Y] + \\text{Cov}[Y,X] + \\text{Cov}[Y, -Y] = \\\\ Var[X] - Cov[X,Y] + Cov[X,Y] - Var[Y] = 0 \\] since \\(Var[X] = Var[Y]\\). 2.3 Weak stationarity, autocovariance and time plot Let X have a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and let \\(Y_t = X\\) for all \\(t\\). Show that \\(\\{Yt\\}\\) is strictly and weakly stationary. Find the autocovariance function for \\(\\{Yt\\}\\). Sketch a “typical” time plot of Yt. We have that \\[ P(Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n}) =\\\\ P(X_1, X_2, \\dots, X_n) =\\\\ P(Y_{t_1 - k}, Y_{t_2 - k}, \\dots, Y_{t_n - k}), \\] which satisfies our requirement for strict stationarity. The autocovariance is given by \\[ \\gamma_{t,s}=\\text{Cov}[Y_t, Y_s] = \\text{Cov}[X,X] = \\text{Var}[X] = \\sigma^2. \\] library(lattice) tstest &lt;- ts(runif(100)) lattice::xyplot(tstest, panel = function(x, y, ...) { panel.abline(h = mean(y), lty = 2) panel.xyplot(x, y, ...) }) Figure 2.1: A white noise time series: no drift, independence between observations. 2.4 Let \\(\\{e_t\\}\\) be a zero mean white noise process. Suppose that the observed process is \\(Y_t = e_t + \\theta e_t − 1\\), where \\(\\theta\\) is either 3 or 1/3. Find the autocorrelation function for \\(\\{Yt\\}\\) both when \\(\\theta = 3\\) and when \\(\\theta = 1/3\\). You should have discovered that the time series is stationary regardless of the value of \\(\\theta\\) and that the autocorrelation functions are the same for \\(\\theta = 3\\) and \\(\\theta = 1/3\\). For simplicity, suppose that the process mean is known to be zero and the variance of \\(Y_t\\) is known to be 1. You observe the series \\(\\{Yt\\}\\) for \\(t = 1, 2, \\dots , n\\) and suppose that you can produce good estimates of the autocorrelations \\(\\rho_k\\). Do you think that you could determine which value of \\(\\theta\\) is correct (3 or 1/3) based on the estimate of \\(\\rho_k\\)? Why or why not? \\[ E[Y_t] = E[e_t+\\theta e_{t-1}] = E[e_t] + \\theta E[e_{t-1}] = 0 + 0 = 0\\\\ V[Y_t] = V[e_t + \\theta e_{t-1}] = V[e_t] + \\theta^2 V[e_{t-1}] = \\sigma_e^2 + \\theta^2 \\sigma_e^2 = \\sigma_2^2(1 + \\theta^2)\\\\ \\] For \\(k = 1\\) we have \\[ C[e_t + \\theta e_{t-1}, e_{t-1} + \\theta e_{t-2}] = \\\\ C[e_t,e_{t-1}] + C[e_t, \\theta e_{t-2}] + C[\\theta e_{t-1}, e_{t-1}] + C[\\theta e_{t-1}, \\theta e_{t-2}] = \\\\ 0 + 0 + \\theta V[e_{t-1}] + 0 = \\theta \\sigma_e^2,\\\\ \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\theta \\sigma_e^2}{\\sqrt{(\\sigma_e^2(1+\\theta^2))^2}} = \\frac{\\theta }{1+\\theta^2} \\] and for \\(k = 0\\) we get \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\text{Corr}[Y_t, Y_t] = 1 \\] and, finally, for \\(k &gt; 0\\): \\[ C[e_t + \\theta e_{t-1}, e_{t-k} + \\theta e_{t-k-1}] = \\\\ C[e_t, e_{t-k}] + C[e_t, e_{t-1-k}] + C[\\theta e_{t-1}, e_{t-k}] + C[\\theta e_{t-1}, \\theta e_{t-1-k}] = 0 \\] given that all terms are independent. Taken together, we have that \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\begin{cases} 1 &amp; \\quad \\text{for } k = 0\\\\ \\frac{\\theta}{1 + \\theta^2} &amp; \\quad \\text{for } k = 1\\\\ 0 &amp; \\quad \\text{for } k &gt; 1 \\end{cases}. \\] And, as required, \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\begin{cases} \\frac{3}{1+3^2} = \\frac{3}{10} &amp; \\quad \\text{if } \\theta = 3\\\\ \\frac{1/3}{1 + (1/3)^2} = \\frac{1}{10/3} = \\frac{3}{10} &amp; \\quad \\text{if } \\theta = 1/3 \\end{cases}. \\] No, probably not. Given that \\(\\rho\\) is standardized, we will not be able to detect any difference in the variance regardless of the values of k. 2.5 Suppose \\(Y_t = 5 + 2t + X_t\\), where \\(\\{X_t\\}\\) is a zero-mean stationary series with autocovariance function \\(\\gamma_k\\). Find the mean function for \\(\\{Y_t\\}\\). Find the autocovariance function for \\(\\{Y_t\\}\\). Is \\(\\{Y_t\\}\\) stationary? Why or why not? \\[\\mu_t = E[Y_t] = E[5 + 2t + X_t] = 5 + 2E[t] + E[X_t] = 5 + 2t + 0 = 2t + 5\\] \\[ \\gamma_k = \\text{Corr}[5+2t+X_t, 5+2(t-k)+X_{t-k}] = \\text{Corr}[X_t, X_{t-k}]\\] No, the mean function (\\(\\mu_t\\)) is constant and the aurocovariance (\\(\\gamma_{t,t-k}\\)) free from \\(t\\). 2.6 Let {Xt} be a stationary time series, and define \\[ Y_t = \\begin{cases} X_t &amp; \\quad \\text{for odd } t \\\\ X_t + 3 &amp; \\quad \\text{for even } t \\end{cases}. \\] Show that \\(\\text{Cov}[Y_t, Y_{t-k}]\\) is free from \\(t\\) for all lags \\(k\\). iS \\(\\{Y_t\\}\\) stationary? \\[\\text{Cov}[a + X_t, b + X_{t-k}] =\\text{Cov}[X_t, X_{t-k}],\\] which is free from \\(t\\) for all \\(k\\) because \\(X_t\\) is stationary. \\[ \\mu_t = E[Y_t] = \\begin{cases} E[X_t] &amp; \\quad \\text{for odd } t\\\\ 3 + E[X_t] &amp; \\quad \\text{for even } t\\\\ \\end{cases}. \\] Since \\(\\mu_t\\) varies depending on \\(t\\), \\(Y_t\\) is not stationary. 2.7 Suppose that \\(\\{Y_t\\}\\) is stationary with autocovariance function \\(\\gamma_k\\). Show that \\(W_t = \\triangledown Y_t = Y_t − Y_t − 1\\) is stationary by finding the mean and autocovariance function for \\(\\{W_t\\}\\). Show that $U_t = 2Y_t = = Y_t − 2Y_t − 1 + Y_t − 2 is stationary. (You need not find the mean and autocovariance function for \\(\\{U_t\\}\\).) \\[\\mu_t = E[W_t] = E[Y_t - Y_{t-1}] = E[Y_t] - E[Y_{t-1}] = 0\\] because \\(Y_t\\) is stationary. \\[ \\text{Cov}[W_t] = \\text{Cov}[Y_t - Y_{t-1}, Y_{t-k} - Y_{t-1-k}] = \\\\ \\text{Cov}[Y_t, Y_{t-k}] + \\text{Cov}[Y_t, Y_{t-1-k}] + \\text{Cov}[-Y_{t-k}, Y_{t-k}] + \\text{Cov}[-Y_{t-k}, -Y_{t-1-k}]=\\\\ \\gamma_k-\\gamma_{k+1}-\\gamma_{k-1}+\\gamma_{k} = 2 \\gamma_k - \\gamma_{k+1} - \\gamma_{k-1}. \\quad \\square \\] In (a), we discovered that the difference between two stationary processes, \\(\\triangledown Y_t\\) itself was stationary. It follows that the difference between two of these differences, \\(\\triangledown^2Y_t\\) is also stationary. 2.8 Suppose that \\(\\{Y_t\\}\\) is stationary with autocovariance function \\(\\gamma_k\\). Show that for any fixed positive integer \\(n\\) and any constants \\(c_1, c_2, \\dots, c_n\\), the process \\(\\{W_t\\}\\) defined by \\(W_t = c_1 Y_t + c_2 Y_{t–1} + \\dots + c_n Y_{t-n+1}\\) is stationary. (Note that Exercise 2.7 is a special case of this result.) \\begin{align} E[W_t] &amp; = c_1E[Y_t]+c_2E[Y_t] + \\dots + c_n E[Y_t]\\\\ &amp; = E[Y_t](c_1 + c_2 + \\dots + c_n), \\end{align} and thus the expected value is constant. Moreover, \\begin{align} \\text{Cov}[W_t] &amp; = \\text{Cov}[c_1 Y_t + c_2 Y_{t-1} + \\dots + c_n Y_{t-k}, c_1 Y_{t-k} + c_2 Y_{t-k-1} + \\dots + c_n Y_{t-k-n}] \\\\ &amp; = \\sum_{i=0}^n \\sum_{j=0}^n c_i c_j \\text{Cov}[Y_{t-j}Y_{t-i-k}] \\\\ &amp; = \\sum_{i=0}^n \\sum_{j=0}^n c_i c_j \\gamma_{j-k-i}, \\end{align} which is free of \\(t\\); consequently, \\(W_t\\) is stationary. "]
]
