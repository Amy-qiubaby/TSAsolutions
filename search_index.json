[
["index.html", "Solutions to Time Series Analysis: with Applications in R Preface Dependencies", " Solutions to Time Series Analysis: with Applications in R Johan Larsson 2017-05-03 Preface This book contains solutions to the problems in the book Time Series Analysis: with Applications in R, second edition, by Cryer and Chan. It is provided as a github repository so that anybody may contribute to its development. Dependencies You will need these packages to reproduce the examples in this book: install.packages(c( &quot;latticeExtra&quot;, &quot;lattice&quot;, &quot;TSA&quot;, &quot;pander&quot;, &quot;gridExtra&quot;, &quot;devtools&quot;, &quot;zoo&quot;, &quot;xts&quot; )) devtools::install_github(&quot;jolars/latticework&quot;) # Load the packages library(devtools) library(lattice) library(latticeExtra) library(gridExtra) library(TSA) library(pander) library(zoo) library(xts) library(latticework) In order for some of the content in this book to be reproducible, the random seed is set at 1234. set.seed(1234) "],
["introduction.html", "1 Introduction 1.1 Larain 1.2 Colors 1.3 Random, normal time series 1.4 Random, \\(\\chi^2\\)-distributed time series 1.5 t(5)-distributed, random values 1.6 Dubuque temperature series", " 1 Introduction 1.1 Larain Use software to produce the time series plot shown in Exhibit 1.2, on page 2. The data are in the file named larain. library(TSA) library(latticeExtra) data(larain, package = &quot;TSA&quot;) xyplot(larain, ylab = &quot;Inches&quot;, xlab = &quot;Year&quot;, type = &quot;o&quot;) 1.2 Colors Produce the time series plot displayed in Exhibit 1.3, on page 3. The data file is named color. data(color) xyplot(color, ylab = &quot;Color property&quot;, xlab = &quot;Batch&quot;, type = &quot;o&quot;) 1.3 Random, normal time series Simulate a completely random process of length 48 with independent, normal values. Plot the time series plot. Does it look “random”? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rnorm(48))) xyplot(as.ts(rnorm(48))) As far as we can tell there is no discernable pattern here. 1.4 Random, \\(\\chi^2\\)-distributed time series Simulate a completely random process of length 48 with independent, chi-square distributed values, each with 2 degrees of freedom. Display the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rchisq(48, 2))) xyplot(as.ts(rchisq(48, 2))) The process appears random, though non-normal. 1.5 t(5)-distributed, random values Simulate a completely random process of length 48 with independent, t-distributed values each with 5 degrees of freedom. Construct the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rt(48, 5))) xyplot(as.ts(rt(48, 5))) It looks random but not normal, though it should be approximately so, considering the distribution that we have sampled from. 1.6 Dubuque temperature series Construct a time series plot with monthly plotting symbols for the Dubuque temperature series as in Exhibit 1.7, on page 6. The data are in the file named tempdub. data(tempdub) xyplot(tempdub, ylab = &quot;Temperature&quot;, xlab = &quot;Year&quot;) "],
["fundamental-concepts.html", "2 Fundamental concepts 2.1 Basic properties of expected value and covariance 2.2 Dependence and covariance 2.3 Strict and weak stationarity 2.4 Zero-mean white noise 2.5 Zero-mean stationary series 2.6 Stationary time series 2.7 First and second-order difference series 2.8 Generalized difference series 2.9 Zero-mean stationary difference series 2.10 Zero-mean, unit-variance process 2.11 Drift 2.12 Periods 2.13 Drift, part 2 2.14 Stationarity, again 2.15 Random variable, zero mean 2.16 Mean and variance 2.17 Variance of sample mean 2.18 Sample variance 2.19 Random walk with drift 2.20 Random walk 2.21 Random walk with random starting value 2.22 Asymptotic stationarity 2.23 Stationarity in sums of stochastic processes 2.24 Measurement noise 2.25 Random cosine wave 2.26 Semivariogram 2.27 Polynomials 2.28 Random cosine wave extended 2.29 Random cosine wave further 2.30 Rayleigh distribution", " 2 Fundamental concepts 2.1 Basic properties of expected value and covariance a \\begin{align} \\text{Cov}[X,Y] &amp; = \\text{Corr}[X,Y]\\sqrt{Var[X]Var[Y]}\\\\ &amp; = 0.25 \\sqrt{9 \\times 4} = 1.5 \\\\ \\text{Var}[X,Y] &amp; = Var[X]+Var[Y]+2Cov[X,Y]\\\\ &amp; = 9 + 4 + 2 \\times 3 = 16\\\\ \\end{align} b \\[\\text{Cov}[X, X+Y] = \\text{Cov}[X,X] + \\text{Cov}[X,Y] = \\text{Var}[X] + \\text{Cov}[X,Y] = 9 + 1.5 = 10.5\\] c \\begin{align} \\text{Corr}[X+Y, X-Y] = &amp; \\text{Corr}[X,X] + \\text{Corr}[X,-Y] + \\text{Corr}[Y,X] + \\text{Corr}[Y,-Y] \\\\ = &amp; \\text{Corr}[Y,X] + \\text{Corr}[Y,-Y] \\\\ = &amp; 1 - 0.25 + 0.25 -1 \\\\ = &amp; 0 \\\\ \\end{align} 2.2 Dependence and covariance \\begin{gather*} \\text{Cov}[X+Y,X-Y] = \\text{Cov}[X,X] + \\text{Cov}[X,-Y] + \\text{Cov}[Y,X] + \\text{Cov}[Y, -Y] = \\\\ Var[X] - Cov[X,Y] + Cov[X,Y] - Var[Y] = 0 \\end{gather*} since \\(Var[X] = Var[Y]\\). 2.3 Strict and weak stationarity a We have that \\begin{gather*} P(Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n}) = \\\\ P(X_1, X_2, \\dots, X_n) = \\\\ P(Y_{t_1 - k}, Y_{t_2 - k}, \\dots, Y_{t_n - k}), \\end{gather*} which satisfies our requirement for strict stationarity. b The autocovariance is given by \\begin{gather*} \\gamma_{t,s}=\\text{Cov}[Y_t, Y_s] = \\text{Cov}[X,X] = \\text{Var}[X] = \\sigma^2. \\end{gather*} c library(lattice) tstest &lt;- ts(runif(100)) lattice::xyplot(tstest, panel = function(x, y, ...) { panel.abline(h = mean(y), lty = 2) panel.xyplot(x, y, ...) }) Figure 2.1: A white noise time series: no drift, independence between observations. 2.4 Zero-mean white noise a \\begin{gather*} E[Y_t] = E[e_t+\\theta e_{t-1}] = E[e_t] + \\theta E[e_{t-1}] = 0 + 0 = 0\\\\ V[Y_t] = V[e_t + \\theta e_{t-1}] = V[e_t] + \\theta^2 V[e_{t-1}] = \\sigma_e^2 + \\theta^2 \\sigma_e^2 = \\sigma_2^2(1 + \\theta^2)\\\\ \\end{gather*} For \\(k = 1\\) we have \\begin{gather*} C[e_t + \\theta e_{t-1}, e_{t-1} + \\theta e_{t-2}] = \\\\ C[e_t,e_{t-1}] + C[e_t, \\theta e_{t-2}] + C[\\theta e_{t-1}, e_{t-1}] + C[\\theta e_{t-1}, \\theta e_{t-2}] = \\\\ 0 + 0 + \\theta V[e_{t-1}] + 0 = \\theta \\sigma_e^2,\\\\ \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\theta \\sigma_e^2}{\\sqrt{(\\sigma_e^2(1+\\theta^2))^2}} = \\frac{\\theta }{1+\\theta^2} \\end{gather*} and for \\(k = 0\\) we get \\begin{gather*} \\text{Corr}[Y_t, Y_{t-k}] = \\text{Corr}[Y_t, Y_t] = 1 \\end{gather*} and, finally, for \\(k &gt; 0\\): \\begin{gather*} C[e_t + \\theta e_{t-1}, e_{t-k} + \\theta e_{t-k-1}] = \\\\ C[e_t, e_{t-k}] + C[e_t, e_{t-1-k}] + C[\\theta e_{t-1}, e_{t-k}] + C[\\theta e_{t-1}, \\theta e_{t-1-k}] = 0 \\end{gather*} given that all terms are independent. Taken together, we have that \\begin{gather*} \\text{Corr}[Y_t, Y_{t-k}] = \\begin{cases} 1 &amp; \\quad \\text{for } k = 0\\\\ \\frac{\\theta}{1 + \\theta^2} &amp; \\quad \\text{for } k = 1\\\\ 0 &amp; \\quad \\text{for } k &gt; 1 \\end{cases}. \\end{gather*} And, as required, \\begin{gather*} \\text{Corr}[Y_t, Y_{t-k}] = \\begin{cases} \\frac{3}{1+3^2} = \\frac{3}{10} &amp; \\quad \\text{if } \\theta = 3\\\\ \\frac{1/3}{1 + (1/3)^2} = \\frac{1}{10/3} = \\frac{3}{10} &amp; \\quad \\text{if } \\theta = 1/3 \\end{cases}. \\end{gather*} b No, probably not. Given that \\(\\rho\\) is standardized, we will not be able to detect any difference in the variance regardless of the values of k. 2.5 Zero-mean stationary series a \\[\\mu_t = E[Y_t] = E[5 + 2t + X_t] = 5 + 2E[t] + E[X_t] = 5 + 2t + 0 = 2t + 5\\] b \\[ \\gamma_k = \\text{Corr}[5+2t+X_t, 5+2(t-k)+X_{t-k}] = \\text{Corr}[X_t, X_{t-k}]\\] c No, the mean function (\\(\\mu_t\\)) is constant and the aurocovariance (\\(\\gamma_{t,t-k}\\)) free from \\(t\\). 2.6 Stationary time series a \\begin{gather*}\\text{Cov}[a + X_t, b + X_{t-k}] =\\text{Cov}[X_t, X_{t-k}],\\end{gather*} which is free from \\(t\\) for all \\(k\\) because \\(X_t\\) is stationary. b \\begin{gather*} \\mu_t = E[Y_t] = \\begin{cases} E[X_t] &amp; \\quad \\text{for odd } t\\\\ 3 + E[X_t] &amp; \\quad \\text{for even } t\\\\ \\end{cases}. \\end{gather*} Since \\(\\mu_t\\) varies depending on \\(t\\), \\(Y_t\\) is not stationary. 2.7 First and second-order difference series a \\begin{gather*}\\mu_t = E[W_t] = E[Y_t - Y_{t-1}] = E[Y_t] - E[Y_{t-1}] = 0\\end{gather*} because \\(Y_t\\) is stationary. \\begin{gather*} \\text{Cov}[W_t] = \\text{Cov}[Y_t - Y_{t-1}, Y_{t-k} - Y_{t-1-k}] = \\\\ \\text{Cov}[Y_t, Y_{t-k}] + \\text{Cov}[Y_t, Y_{t-1-k}] + \\text{Cov}[-Y_{t-k}, Y_{t-k}] + \\text{Cov}[-Y_{t-k}, -Y_{t-1-k}]=\\\\ \\gamma_k-\\gamma_{k+1}-\\gamma_{k-1}+\\gamma_{k} = 2 \\gamma_k - \\gamma_{k+1} - \\gamma_{k-1}. \\quad \\square \\end{gather*} b In (a), we discovered that the difference between two stationary processes, \\(\\triangledown Y_t\\) itself was stationary. It follows that the difference between two of these differences, \\(\\triangledown^2Y_t\\) is also stationary. 2.8 Generalized difference series \\begin{align} E[W_t] &amp; = c_1E[Y_t]+c_2E[Y_t] + \\dots + c_n E[Y_t]\\\\ &amp; = E[Y_t](c_1 + c_2 + \\dots + c_n), \\end{align} and thus the expected value is constant. Moreover, \\begin{align} \\text{Cov}[W_t] &amp; = \\text{Cov}[c_1 Y_t + c_2 Y_{t-1} + \\dots + c_n Y_{t-k}, c_1 Y_{t-k} + c_2 Y_{t-k-1} + \\dots + c_n Y_{t-k-n}] \\\\ &amp; = \\sum_{i=0}^n \\sum_{j=0}^n c_i c_j \\text{Cov}[Y_{t-j}Y_{t-i-k}] \\\\ &amp; = \\sum_{i=0}^n \\sum_{j=0}^n c_i c_j \\gamma_{j-k-i}, \\end{align} which is free of \\(t\\); consequently, \\(W_t\\) is stationary. 2.9 Zero-mean stationary difference series a \\begin{gather*} E[Y_t] = \\beta_0 + \\beta_1 t + E[X_t] = \\beta_0 + \\beta_1 t + \\mu_{t_x}, \\end{gather*} which is not free of \\(t\\) and hence not stationary. \\begin{gather*} \\text{Cov}[Y_t] = \\text{Cov}[X_t, X_t-1] = \\gamma_{t-1} \\end{gather*} \\begin{gather*} E[W_t] = E[Y_t - Y_{t-1}] = E[\\beta_0 + \\beta_1 t + X_t - (\\beta_0 + \\beta_1(t-1) + X_{t-1})] =\\\\ \\beta_0 + \\beta_1 t - \\beta_0 - \\beta_1 t + \\beta_1 = \\beta_1, \\end{gather*} is free of \\(t\\) and, furthermore, we have \\begin{gather*} \\text{Cov}[W_t] = \\text{Cov}[\\beta_0 + \\beta_1 t + X_t, \\beta_0 + \\beta_1 (t-1) + X_{t-1}] =\\\\ \\text{Cov}[X_t, X_{t-1}] = \\gamma_k, \\end{gather*} which is also free of \\(t\\), thereby proving that \\(W_t\\) is stationary. b \\begin{gather*} E[Y_t] = E[\\mu_t + X_t] = \\mu_t + \\mu_t = 0 + 0 = 0, \\quad \\text{and}\\\\ \\text{Cov}[Y_t] = \\text{Cov}[\\mu_t + X_t, \\mu_{t-k} + X_{t-k}] = \\text{Cov}[X_t, X_{t-k}] = \\gamma_k \\end{gather*} \\begin{gather*} \\triangledown^m Y_t = \\triangledown(\\triangledown^{m−1}Y_t) \\end{gather*} Currently unsolved. 2.10 Zero-mean, unit-variance process a \\begin{gather*} \\mu_t = E[Y_t] = E[\\mu_t + \\sigma_t X_t] = \\mu_t + \\sigma_t E[X_t] = \\mu_t + \\sigma_t \\times 0 = \\mu_t\\\\ \\gamma_{t,t-k} = \\text{Cov}[Y_t] = \\text{Cov}[\\mu_t + \\sigma_t X_t, \\mu_{t-k} + \\sigma_{t-k} X_{t-k}] = \\sigma_t \\sigma_{t-k} \\text{Cov}[X_t, X_{t-k}] = \\sigma_t \\sigma_{t-k} \\rho_k \\end{gather*} b First, we have \\begin{gather*} \\text{Var}[Y_t] = \\text{Var}[\\mu_t + \\sigma_t X_t] = 0 + \\sigma_t^2 \\text{Var}[X_t] = \\sigma_t^2 \\times 1 = \\sigma_t^2 \\end{gather*} since \\(\\{X_t\\}\\) has unit-variance. Futhermore, \\begin{gather*} \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\sigma_t \\sigma_{t-k} \\rho_k}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t-k}]}} = \\frac{\\sigma_t \\sigma_{t-k}\\rho_k}{\\sigma_t \\sigma_{t-k}} = \\rho_k, \\end{gather*} which depends only on the time lag, \\(k\\). However, \\(\\{Y_t\\}\\) is not necessarily stationary since \\(\\mu_t\\) may depend on \\(t\\). c Yes, \\(\\rho_k\\) might be free from \\(t\\) but if \\(\\sigma_t\\) is not, we will have a non-stationary time series with autocorrelation free from \\(t\\) and constant mean. 2.11 Drift a \\begin{gather*} \\text{Cov}[X_t, X_{t-k}] = \\gamma_k\\\\ E[X_t] = 3t \\end{gather*} \\(\\{X_t\\}\\) is not stationary because \\(\\mu_t\\) varies with \\(t\\). b \\begin{gather*} E[Y_t] = 3 - 3t+E[X_t] = 7 - 3t - 3t = 7\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[7-3t+X_t,7-3(t-k)+X_{t-k}] = \\text{Cov}[X_t, X_{t-k}] = \\gamma_k \\end{gather*} Since the mean function of \\(\\{Y_t\\}\\) is constant (7) and its autocovariance free of \\(t\\), \\(\\{Y_t\\}\\) is stionary. 2.12 Periods \\begin{gather*} E[Y_t] = E[e_t - e_{t-12}] = E[e_t] - E[e_{t-12}] = 0\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[e_t - e_{t-12}, e_{t-k} - e_{t-12-k}] =\\\\ \\text{Cov}[e_t, e_{t-k}] - \\text{Cov}[e_t, e_{t-12-k}] - \\text{Cov}[e_{t-12}, e_{t-k}] + \\text{Cov}[e_{t-12}, e_{t-12-k}] \\end{gather*} Then, as required, we have \\begin{gather*} \\text{Cov}[Y_t, Y_{t-k}] = \\begin{cases} \\text{Cov}[e_t, e_{t-12}] - \\text{Cov}[e_t, e_t] -\\\\ \\text{Cov}[e_{t-12}, e_{t-12}] + \\text{Cov}[e_{t-12},e_t] =\\\\ \\text{Var}[e_t] - \\text{Var}[e_{t-12}] \\neq 0 &amp; \\quad \\text{for } k=12\\\\ \\\\ \\text{Cov}[e_t, e_{t-k}] - \\text{Cov}[e_t, e_{t-12-k}] -\\\\ \\text{Cov}[e_{t-12}, e_{t-k}] + \\text{Cov}[e_{t-12}, e_{t-12-k}] =\\\\ 0 + 0 + 0 + 0 = 0 &amp; \\quad \\text{for } k \\neq 12 \\end{cases} \\end{gather*} 2.13 Drift, part 2 a \\begin{gather*} E[Y_t] = E[e_t - \\theta e_{t-1}^2] = E[e_t] - \\theta E[e_{t-1}^2] = 0 - \\theta \\text{Var}[e_{t-1}] = -\\theta \\sigma_e^2 \\end{gather*} And thus the requirement of constant variance is fulfilled. Moreover, \\begin{gather*} \\text{Var}[Y_t] = \\text{Var}[e_t-\\theta e_{t-1}^2] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t-1}^2] = \\sigma_e^2 + \\theta^2 (E[e_{t-1}^4] - E[e_{t-1}^2]^2), \\end{gather*} where \\begin{gather*} E[e_{t-1}^4] = 3\\sigma_e^4 \\quad \\text{and} \\quad E[e_{t-1}^2 ]^2 = \\sigma_e^4, \\end{gather*} gives us \\begin{gather*} \\text{Var}[Y_t] = \\sigma_e^2 + \\theta(3\\sigma_e^4 - \\sigma_e^2) = \\sigma_e^2 + 2 \\theta^2 \\sigma_e^4 \\end{gather*} and \\begin{gather*} \\text{Cov}[Y_t, Y_{t-1}] = \\text{Cov}[e_t - \\theta e_{t-1}^2, e_{t-1} - \\theta e_{t-2}^2] = \\\\ \\text{Cov}[e_t, e_{t-1}] + \\text{Cov}[e_t, - \\theta e_{t-2}^2] + \\text{Cov}[- \\theta e_{t-1}^2, e_{t-1}] \\text{Cov}[-\\theta e_{t-1}^2, - \\theta e_{t-2}^2] =\\\\ \\text{Cov}[e_t, e_{t-1}] - \\theta \\text{Cov}[e_t, e_{t-2}^2] - \\theta \\text{Cov}[e_{t-1}^2, e_{t-1}] + \\theta^2 \\text{Cov}[e_{t-1}^2, e_{t-2}^2] = \\\\ -\\theta \\text{Cov}[e_{t-1}^2, e_{t-1}] = -\\theta (E[e_{t-1}^3] + \\mu_{t-1} + \\mu_t) = 0 \\end{gather*} which means that the autocorrelation function \\(\\gamma_{t,s}\\) also has to be zero. b The autocorrelation of \\(\\{Y_t\\}\\) is zeor and its mean function is constant, thus \\(\\{Y_t\\}\\) must be stationary. 2.14 Stationarity, again a \\begin{gather*} E[Y_t]= E[\\theta_0 + t e_t] = \\theta_0 + E[e_t] = \\theta_0+t \\times 0 = \\theta_0\\\\ \\text{Var}[Y_t] = \\text{Var}[\\theta_0] + \\text{Var}[t e_t] = 0 + t^2\\sigma_e^2 = t^2\\sigma_e^2 \\end{gather*} So \\(\\{Y_t\\}\\) is not stationary. b \\begin{gather*} E[W_t] = E[\\triangledown Y_t] = E[\\theta_0 + te_t - \\theta_0 - (t-1)e_{t-1}] = tE[e_t] - tE[e_{t-1} + E[e_{t-1}] = 0 \\\\ \\text{Var}[\\triangledown Y_t] = \\text{Var}[t e_t] = - \\text{Var}[(t-1)e_{t-1}] = t^2 \\sigma_e^2 - (t-1)^2 \\sigma_e^2 = \\sigma_e^2 (t^2 - t^2 + 2t - 1) = (2t-1)\\sigma_e^2, \\end{gather*} which varies with \\(t\\) and means that \\(\\{W_t\\}\\) is not stationary. c \\begin{gather*} E[Y_t] = E[e_t e_{t-1}] = E[e_t] E[e_{t-1}] = 0\\\\ \\text{Cov}[Y_t, Y_{t-1}] = \\text{Cov}[e_t e_{t-1}, e_{t-1} e_{t-2}] = E[(e_t e_{t-1} - \\mu_t^2)(e_{t-1} e_{t-2} - \\mu_t^2)] =\\\\ E[e_t]E[e_{t-1}]E[e_{t-1}]E[e_{t-2}] = 0 \\end{gather*} Both the covariance and the mean function are zero, hence the process is stationary. 2.15 Random variable, zero mean a \\(E[Y_t] = (-1)^tE[X] = 0\\) b \\(\\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[(-1)^tX, (-1)^{t-k}X] = (-1)^{2t-k}\\text{Cov}[X, X] = (-1)^k \\text{Var}[X] = (-1)^k\\sigma_t^2\\) c Yes, the covariance is free of \\(t\\) and the mean is constant. 2.16 Mean and variance \\begin{gather*} E[Y_t] = E[A + X_t] = E[A] + E[X_t] = \\mu_A + \\mu_X\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[A + X_t, A+ X_{t-k}] = \\\\ \\text{Cov}[A, A] + \\text{Cov}[A, X_{t-k}] + \\text{Cov}[X_t, A] + \\text{Cov}[X_t, X_{t-k}] = \\sigma_A^2 + \\gamma_{k_k} \\end{gather*} 2.17 Variance of sample mean \\begin{gather*} \\text{Var}[\\bar{Y}] = \\text{Var}\\left[ \\frac{1}{n} \\sum_{t=1}^n Y_t \\right] = \\frac{1}{n^2} \\text{Var}\\left[ \\sum_{t=1}^n Y_t \\right] = \\\\ \\frac{1}{n^2}\\text{Cov}\\left[ \\sum_{t=1}^n Y_t, \\sum_{s=1}^n Y_s \\right] = \\frac{1}{n^2} \\sum_{t=1}^n \\sum_{s=1}^n \\gamma_{t-s} \\end{gather*} Setting \\(k = t-s, j = t\\) gives us \\begin{gather*} \\text{Var}[\\bar{Y}] = \\frac{1}{n^2} \\sum_{j=1}^n \\sum_{j-k=1}^n \\gamma_k = \\frac{1}{n^2} \\sum_{j=1}^n \\sum_{j=k+1}^{n+k} \\gamma_k = \\\\ \\frac{1}{n^2} \\left( \\sum_{k=1}^{n-1} \\sum_{j=k+1}^{n} \\gamma_k + \\sum_{k=-n+1}^0 \\sum_{j=1}^{n+k} \\gamma_k \\right) = \\\\ \\frac{1}{n^2} \\left( \\sum_{k=1}^{n-1} (n-k)\\gamma_k + \\sum_{k=-n+1}^0 (n+k)\\gamma_k \\right) = \\\\ \\frac{1}{n^2} \\sum_{k=-n+1}^{n-1} \\left( (n-k)\\gamma_k + (n+k)\\gamma_k \\right) = \\\\ \\frac{1}{n^2} \\sum_{k=-n+1}^{n-1} (n-|k|)\\gamma_k = \\frac{1}{n} \\sum_{k=-n+1}^{n-1} \\left(1-\\frac{|k|}{n}\\right)\\gamma_k \\quad \\square \\end{gather*} 2.18 Sample variance a \\begin{gather*} \\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t=1}^n((Y_t - \\bar{Y}) + (\\bar{Y} - \\mu))^2 = \\\\ \\sum_{t=1}^n ((Y_t - \\bar{Y})^2 - 2(Y_t - \\bar{Y})(\\bar{Y}- \\mu) + (\\bar{Y} - \\mu)^2) = \\\\ n(\\bar{Y} - \\mu)^2 + 2(\\bar{Y} - \\mu)\\sum_{t=1}^n (Y_t - \\bar{Y}) + \\sum_{t=1}^n (Y_t - \\bar{Y})^2 = \\\\ n(\\bar{Y} - \\mu)^2 + \\sum_{t=1}^n(Y_t - \\bar{Y})^2 \\quad \\square \\end{gather*} b \\begin{gather*} E[s^2] = E\\left[\\frac{n}{n-1} \\sum_{t=1}^n (Y_t - \\bar{Y})^2 \\right] = \\frac{n}{n-1} E\\left[\\sum_{t=1}^n \\left( (Y_t-\\mu)^2 + n(\\bar{Y} - \\mu)^2 \\right)\\right] = \\\\ \\frac{n}{n-1} \\sum_{t=1}^n \\left( E[(Y_t-\\mu)^2] + nE[(\\bar{Y} - \\mu)^2] \\right) = \\frac{1}{n-1} \\left( n\\text{Var}[Y_t] - n\\text{Var}[\\bar{Y}] \\right) = \\\\ \\frac{n}{n-1} \\gamma_0 - \\frac{n}{n-1} \\text{Var}[\\bar{Y}] = \\frac{1}{n-1} \\left( n \\gamma_0 - n \\left( \\frac{\\gamma_0}{n} + \\frac{2}{n} \\sum_{k=1}^{n-1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k\\right) \\right) = \\\\ \\frac{1}{n-1} \\left( n \\gamma_0 - \\gamma_0 + 2 \\sum_{k=1}^{n-1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k\\right) = \\frac{1}{n-1} \\left( \\gamma_0(n-1) + 2 \\sum_{k=1}^{n-1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k\\right) = \\\\ \\gamma_0 + \\frac{2}{n-1} \\sum_{k=1}^{n-1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k \\quad \\square \\end{gather*} c Since \\(\\gamma_k = 0\\) for \\(k \\neq 0\\), in our case for all \\(k\\), we have \\begin{gather*} E[s^2] = \\gamma_0 - \\frac{2}{n-1} \\sum_{t=1}^n \\left( 1 - \\frac{k}{n} \\right) \\times 0 = \\gamma_0 \\end{gather*} 2.19 Random walk with drift a \\begin{gather*} Y_{1} = \\theta_0 + e_1\\\\ Y_{2} = \\theta_0 + \\theta_0 + e_2 + e_1\\\\ Y_{t} = \\theta_0 + \\theta_0 + \\dots + \\theta_0 + e_{t} + e_{t-1} + \\dots+ e_1 = \\\\ Y_{t} = t \\theta_0 + e_t + e_{t-1} + \\dots + e_1 \\quad \\square \\end{gather*} b \\begin{gather*} \\mu_t = E[Y_t] = E[t \\theta_0 + e_t + e_{t-1} + \\dots + e_1] = t\\theta_0 + E[e_t] + E[e_{t-1}] + \\dots + E[e_1] = \\\\ t\\theta_0 + 0 + 0 + \\dots + 0 = t \\theta_0 \\end{gather*} c \\begin{gather*} \\gamma_{t,t-k} = \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[t\\theta_0 + e_t, + e_{t-1} + \\dots + e_1, (t-k)\\theta_0 + e_{t-k}, + e_{t-1-k} + \\dots + e_1] = \\\\ \\text{Cov}[e_{t-k}, + e_{t-1-k} + \\dots + e_1, e_{t-k}, + e_{t-1-k} + \\dots + e_1] \\quad \\text{(since all other terms are 0)} =\\\\ \\text{Var}[e_{t-k}, + e_{t-1-k} + \\dots + e_1, e_{t-k}, + e_{t-1-k} + \\dots + e_1] = (t-k)\\sigma_e^2 \\end{gather*} 2.20 Random walk a \\begin{gather*} \\mu_1 = E[Y_1] = E[e_1] = 0\\\\ \\mu_2 = E[Y_2] = E[Y_1 - e_2] = E[Y_1] - E[e_2] = 0 - 0 = 0\\\\ \\dots\\\\ \\mu_{t-1} = E[Y_{t-1}] = E[Y_{t-2} - e_{t-1}] = E[Y_{t-2}] - E[e_{t-1}] = 0 \\\\ \\mu_t = E[Y_t] = E[Y_{t-1} - e_t] = E[Y_t] - E[e_t] = 0, \\end{gather*} which implies \\(\\mu_t = \\mu_{t-1}\\quad\\) Q.E.D. b \\begin{gather*} \\text{Var}[Y_1] = \\sigma_e^2\\\\ \\text{Var}[Y_2] = \\text{Var}[Y_1 - e_2] = \\text{Var}[Y_1] + \\text{Var}[e_1] = \\sigma_e^2 + \\sigma_e^2 = 2\\sigma_e^2\\\\ \\dots\\\\ \\text{Var}[Y_{t-1}] = \\text{Var}[Y_{t-2} - e_{t-1}] = \\text{Var}[Y_{t-2}] + \\text{Var}[e_{t-1}] = (t-1)\\sigma_e^2\\\\ \\text{Var}[Y_t] = \\text{Var}[Y_{t-1} - e_t] = \\text{Var}[Y_{t-1}] + \\text{Var}[e_t] = (t-1)\\sigma_e^2 + \\sigma_e^2 = t\\sigma_e^2 \\quad \\square \\end{gather*} c \\begin{gather*} \\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_t, Y_t+e_{t+1}+e_{t+2}+ \\dots + e_s] = \\text{Cov}[Y_t, Y_t] = \\text{Var}[Y_t] = t\\sigma_e^2 \\end{gather*} 2.21 Random walk with random starting value a \\begin{gather*} E[Y_t] = E[Y_0+e_t+e_{t-1}+\\dots+e_1] = \\\\ E[Y_0] + E[e_t] + E[e_{t-1}] + E[e_{t-2}] + \\dots + E[e_1] = \\\\ \\mu_0 + 0 + \\dots + 0 = \\mu_0 \\quad \\square \\end{gather*} b \\begin{gather*} \\text{Var}[Y_t] = \\text{Var}[Y_0 + e_t + e_{t-1} + \\dots + e_1] = \\\\ \\text{Var}[Y_0] + \\text{Var}[e_t] + \\text{Var}[e_{t-1}] + \\dots + \\text{Var}[e_1] = \\\\ \\sigma_0^2+t\\sigma_e^2 \\quad \\square \\end{gather*} c \\begin{gather*} \\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_t, Y_t+e_{t+1}+e_{t+2}+ \\dots + e_s] = \\\\ \\text{Cov}[Y_t, Y_t] = \\text{Var}[Y_t] = \\sigma_0^2+t\\sigma_e^2 \\quad \\square \\end{gather*} d \\begin{gather*} \\text{Corr}[Y_t, Y_s] = \\frac{\\sigma_0^2+t\\sigma_e^2}{\\sqrt{(\\sigma_0^2+t\\sigma_e^2)(\\sigma_0^2+s\\sigma_e^2)}} = \\sqrt{\\frac{\\sigma_0^2+t\\sigma_e^2}{\\sigma_0^2+s\\sigma_e^2}} \\quad \\square \\end{gather*} 2.22 Asymptotic stationarity a \\begin{gather*} E[Y_1] = E[e_1] = 0\\\\ E[Y_2] = E[cY_{1}+e_2] = cE[Y_1] + E[e_2] = 0\\\\ \\dots\\\\ E[Y_t] = E[cY_{t-1}+e_t] = cE[Y_{t-1}] + E[e_t] = 0\\quad \\square \\end{gather*} b \\begin{gather*} \\text{Var}[Y_1] = \\text{Var}[e_1] = \\sigma_e^2\\\\ \\text{Var}[Y_2] = \\text{Var}[cY_{1} + e_2] = c^2\\text{Var}[Y_{t-1}] + \\text{Var}[e_2] = c^2\\sigma_e^2 + \\sigma_e^2 = \\sigma_e^2(1 + c^2)\\\\ \\dots\\\\ \\text{Var}[Y_t] = \\sigma_e^2(1 + c^2 + c^4 + \\dots + c^{2t-2}) \\quad\\square \\end{gather*} \\(\\{Y_t\\}\\) is not stationary, given that its variance varies with \\(t\\). c \\begin{gather*} \\text{Cov}[Y_t, Y_{t-1}] = \\text{Cov}[cY_{t-1} + e_t, Y_{t-1}] = c\\text{Cov}[Y_{t-1}, Y_{t-1}] = c\\text{Var}[Y_{t-1}]\\quad \\text{giving}\\\\ \\text{Corr}[Y_t, Y_{t-1}] = \\frac{c\\text{Var}[Y_{t-1}]}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t-1}]}} = c \\sqrt{\\frac{\\text{Var}[Y_{t-1}]}{\\text{Var}[Y_t]}}\\quad\\square \\end{gather*} And, in the general case, \\begin{gather*} \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[cY_{t-1}+e_t, Y_{t-k}] = \\\\ c\\text{Cov}[cY_{t-2} + e_{t-1}, Y_{t-k}] =\\\\ c^3\\text{Cov}[Y_{t-2} + e_{t-1}, Y_{t-k}] = \\dots\\\\ = c^k\\text{Var}[Y_{t-k}] \\end{gather*} giving \\begin{gather*} \\text{Corr}[Y_t, Y_{t-k}] = \\frac{c^k\\text{Var}[Y_{t-k}]}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t-k}]}} = c^k \\sqrt{\\frac{\\text{Var}[Y_{t-k}]}{\\text{Var}[Y_t]}}\\quad\\square \\end{gather*} d \\begin{gather*} \\text{Var}[Y_t] = \\sigma_e^2(1+c^2+c^4+\\dots+c^{2t-2}) = \\sigma_e^2\\sum_{t=1}^{n}c^{2(t-1)}=\\sigma_e^2 \\sum_{t=0}^{n-1} c^{2t} = \\sigma_e^2 \\frac{1-c^{2t}}{1-c^2} \\end{gather*} And because \\begin{gather*} \\lim_{t \\rightarrow \\infty} \\sigma_e^2 \\frac{1-c^{2t}}{1-c^2} = \\sigma_e^2 \\frac{1}{1-c^2}\\quad\\text{since }|c| &lt; 1, \\end{gather*} which is free of \\(t\\), \\(\\{Y_t\\}\\) can be considered asymptotically stationary. e \\begin{gather*} Y_t = c(cY_{t-2} + e_{t-1}) + e_t = \\dots = e_t+ce_{t-1} + c^2e_{t-2} + \\dots + c^{t-2}e_2+ \\frac{c^{t-1}}{\\sqrt{1-c^2}}e_1\\\\ \\text{Var}[Y_t] = \\text{Var}[e_t+ce_{t-1}+c^2e_{t-2}+\\dots+c^{t-2}e_2+\\frac{c^{t-1}}{\\sqrt{1-c^2}}e_1] =\\\\ \\text{Var}[e_t] + c^2\\text{Var}[e_{t-1}]+c^4 \\text{Var}[e_{t-2}] + \\dots + c^{2(t-2)}\\text{Var}[e_2]+\\frac{c^{2(t-1)}}{1-c^2}\\text{Var}[e_1] =\\\\ \\sigma_e^2(1 + c^2 + c^4 + \\dots + c^{2(t-2)} + \\frac{c^{2(t-1)}}{1-c^2}) =\\sigma_e^2\\left( \\sum_{t=1}^{n}c^{2(t-1)} - c^{2(t-1)} + \\frac{c^{2(t-1)}}{1-c^2}\\right)= \\\\ \\sigma_e^2 \\frac{1-c^{2t}+c^{2t-2+2}}{1-c^2} = \\sigma_e^2 \\frac{1}{1-c^2} \\quad \\square \\end{gather*} Futhermore, \\begin{gather*} E[Y_1] = E\\left[\\frac{e_1}{\\sqrt{1-c^2}}\\right] = \\frac{E[e_1]}{\\sqrt{1-c^2}} = 0\\\\ E[Y_2] = E[cY_{1} + e_2] = cE[Y_{1}] = 0\\\\ \\dots \\\\ E[Y_t] = E[cY_{t-1} + e_2] = cE[Y_{t-1}] = 0,\\\\ \\end{gather*} which satisfies our first requirement for weak stationarity. Also, \\begin{gather*} \\text{Cov}[Y_t,Y_{t-k}] = \\text{Cov}[cY_{t-1} + e_t, Y_{t-1}] = c^k\\text{Var}[Y_{t-1}] =\\\\ c^k \\frac{\\sigma_e^2}{1-c^2}, \\end{gather*} which is free of \\(t\\) and hence \\(\\{Y_t\\}\\) is now stationary. 2.23 Stationarity in sums of stochastic processes \\begin{gather*} E[W_t] = E[Z_t + Y_t] = E[Z_t] + Y[Z_t] = \\mu_{Z_t} + \\mu_{Y_s} \\end{gather*} Since both processes are stationary – and hence their sums are constant – the sum of both processes must also be constant. \\begin{gather*} \\text{Cov}[W_t, W_{t-k}] = \\text{Cov}[Z_t + Y_t, Z_{t-k} + Y_{t-k}] = \\\\ \\text{Cov}[Z_t, Z_{t-k}] + \\text{Cov}[Z_t, Y_{t-k}] + \\text{Cov}[Y_t, Z_{t-k}] + \\text{Cov}[Y_t, Y_{t-k}] = \\\\ \\text{Cov}[Z_t, Z_{t-k}] + \\text{Cov}[Z_t, Y_{t-k}] + \\text{Cov}[Y_t, Z_{t-k}] + \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[Z_t, Z_{t-k}] + \\text{Cov}[Y_t, Y_{t-k}] = \\gamma_{Z_k} + \\gamma_{Y_k}, \\end{gather*} both free of \\(t\\). 2.24 Measurement noise \\begin{gather*} E[Y_t] = E[Y_t + e_t] = E[X_t] + E[e_t] - \\mu_t\\\\ \\text{Var}[Y_t] = \\text{Var}[X_t + e_t] = \\text{Var}[X_t]+\\text{Var}[e_t] = \\sigma_X^2 + \\sigma_e^2\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[X_t + e_t, X_{t-k}+e_{t-k}] = \\text{Cov}[X_t, X_{t-k}] = \\rho_k\\\\ \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\rho_k}{\\sqrt{(\\sigma_X^2 + \\sigma_e^2)(\\sigma_X^2 + \\sigma_e^2)}} = \\frac{\\rho_k}{\\sigma_X^2 + \\sigma_e^2} = \\frac{\\rho_k}{1 + \\frac{\\sigma_e^2}{\\sigma_X^2}} \\quad \\square \\end{gather*} 2.25 Random cosine wave \\begin{gather*} E[Y_t] = E\\left[\\beta_0 + \\sum_{i=1}^k(A_i\\cos(2\\pi f_it) + B_i \\sin(2\\pi f_it))\\right] = \\\\ \\beta_0 + \\sum_{i=1}^k(E[A_i]\\cos(2\\pi f_it) + E[B_i]\\sin(2\\pi f_it) = \\beta_0\\\\ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[\\sum_{i=1}^k A_i\\cos(2\\pi f_it) + B_i\\sin(2\\pi f_it), \\sum_{j=1}^k A_j\\cos(2\\pi f_j s) + B_j\\sin(2\\pi f_j s)\\right] =\\\\ \\sum_{i=1}^k \\text{Cov}[A_i\\cos(2\\pi f_it) + A_i\\sin(2\\pi f_is)] + \\sum_{i=1}^k \\text{Cov}[B_i\\cos(2\\pi f_j t) + B_i\\sin(2\\pi f_j s)] = \\\\ \\sum_{i=1}^k \\text{Var}[A_i](\\cos(2\\pi f_it) + \\sin(2\\pi f_is)) + \\sum_{i=1}^k \\text{Var}[B_i](\\cos(2\\pi f_j t) + \\sin(2\\pi f_j s)) = \\\\ \\frac{\\sigma_i^2}{2} \\sum_{i=1}^k (\\cos(2\\pi f_i (t-s)) + \\sin(2\\pi f_i (t+s))) + \\frac{\\sigma_i^2}{2} \\sum_{i=1}^k (\\cos(2\\pi f_j (t-s)) + \\sin(2\\pi f_j (t+s))) = \\\\ \\sigma_i^2 \\sum_{i=1}^k \\cos(2\\pi f_i (t-s)) = \\sigma_i^2 \\sum_{i=1}^k \\cos(2\\pi f_i k), \\end{gather*} and is thus free of \\(t\\) and \\(s\\). 2.26 Semivariogram a \\begin{gather*} \\Gamma_{t,s} = \\frac{1}{2}E[(Y_t-Y_s)^2] = \\frac{1}{2}E[Y_t^2 - 2Y_t Y_s + Y_s^2] = \\\\ \\frac{1}{2}\\left( E[Y_t^2] - 2E[Y_t Y_s] + E[Y_s^2] \\right) = \\frac{1}{2}\\gamma_0 + \\frac{1}{2}\\gamma_0 - 2 \\times \\frac{1}{2}\\gamma_{|t-s|} = \\gamma_0 - \\gamma_{|t-s|}\\\\ \\text{Cov}[Y_t,Y_s] = E[Y_tY_s]-\\mu_t\\mu_s=E[Y_tY_s]=\\gamma_{|t-s|} \\quad \\square \\end{gather*} b \\begin{gather*} Y_t-Y_s = e_t + e_{t-1} + \\dots + e_1 - e_s - e_{s-1} - \\dots - e_1 = \\\\ e_t + e_{t-1} + \\dots + e_{s+1}, \\quad \\text{for } t &gt; s \\\\ \\Gamma_{t,s} = \\frac{1}{2}E[(Y_t-Y_s)^2] = \\frac{1}{2}\\text{Var}[e_t + e_{t-1} + \\dots + e_{s-1}] =\\\\ \\frac{1}{2}\\sigma_e^2(t-s) \\quad \\square \\end{gather*} 2.27 Polynomials a \\begin{gather*} E[Y_t] = E[e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\dots + \\phi^r e_{t-r}] = 0\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[e_t + \\phi e_{t-1} + \\dots + \\phi^r e_{t-r}, e_{t-k} + \\phi e_{t-1-k} + \\dots + \\phi^r e_{t-r-k}] =\\\\ \\text{Cov}[e_1+\\dots + \\phi^k e_{t-k} + \\phi^{k+1}e_{t-k-1} + \\dots + \\phi^r e_{t-r}, e_{t-r}, e_{t-k} + \\dots + \\phi^k e_{t-k-1} + \\dots + \\phi^r e_{t-k-r}] = \\\\ \\sigma_e^2(\\phi^k + \\phi^{k+2} + \\phi^{k+4} + \\dots + \\phi^{k+2(r-k)}) = \\sigma_e^2 \\phi^k(1 + \\phi^2 + \\phi^4 + \\dots + \\phi^{2(r-k)}) \\end{gather*} Hence, because of the zero mean and covariance free of \\(t\\), it is a stationary process. b \\begin{gather*} \\text{Var}[Y_t] = \\text{Var}[e_t + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\dots + \\phi^r e_{t-r}] = \\sigma_e^2(1 + \\phi + \\phi^2 + \\dots + \\phi^{2r})\\\\ \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\sigma_e^2 \\phi^k(1 + \\phi^2 + \\phi^4 + \\dots + \\phi^{2(r-k)})}{\\sqrt{(\\sigma_e^2(1 + \\phi + \\phi^2 + \\dots + \\phi^{2r}))^2}} = \\frac{\\phi^k(1 + \\phi^2 + \\phi^4 + \\dots + \\phi^{2(r-k)})}{(1 + \\phi + \\phi^2 + \\dots + \\phi^{2r})} \\quad \\square \\end{gather*} 2.28 Random cosine wave extended a \\begin{gather*} E[Y_t] = E[R \\cos{(2\\pi(ft+\\phi))}] = E[R] \\cos{(2\\pi(ft+\\phi))} = \\\\ E[R] \\int_0^1\\cos(E[R \\cos(2\\pi(ft+\\phi))])d\\phi = E[R]\\left[ \\frac{1}{2\\pi}\\sin(2\\pi(ft+\\phi))\\right]^1_0 = \\\\ E[R] \\left( \\frac{1}{2\\pi}(\\sin(2\\pi(ft+1)) - \\sin(2\\pi(ft))) \\right) = \\\\ E[R] \\left( \\frac{1}{2\\pi}(\\sin(2\\pi ft + 2\\pi) - \\sin(2\\pi ft + 1)) \\right) = \\\\ E[R] \\left( 0 \\right) = 0 \\end{gather*} b \\begin{gather*} \\gamma_{t,s} = E[R \\cos{(2\\pi(ft+\\phi))} R \\cos{(2\\pi(fs+\\phi))}] = \\\\ \\frac{1}{2} E[R^2] \\int_0^1\\left(\\cos{\\left(2\\pi(f(t-s)\\right)} + \\frac{1}{4\\pi}\\sin{(2\\pi(f(t+s) + 2\\phi)}) \\right) =\\\\ \\frac{1}{2} E[R^2]\\left[ \\cos{(2\\pi f(t-s))} + \\frac{1}{4\\pi}\\sin{(2\\pi(f(t+s) + 2\\phi))} \\right]^1_0 = \\\\ \\frac{1}{2} E[R^2]\\left( \\cos{(2\\pi (f|t-s|))} \\right), \\end{gather*} which is free of \\(t\\). 2.29 Random cosine wave further a \\begin{gather*} E[Y_t] = \\sum_{j=1}^m E[R_j]E[\\cos{(2\\pi(f_j t+\\phi))}] = \\text{via 2.28} = \\sum_{j=1}^m E[R_j] \\times 0 = 0 \\end{gather*} b \\begin{gather*} \\gamma_k = \\sum_{j=1}^m E[R_j]\\cos{(2\\pi f_jk)}, \\text{ also from 2.28.} \\end{gather*} 2.30 Rayleigh distribution \\begin{gather*} Y = R\\cos{(2\\pi(ft + \\phi))}, \\quad X = R\\sin{(2\\pi(ft+\\phi))}\\\\ \\begin{bmatrix} \\frac{\\partial X}{\\partial R} &amp; \\frac{\\partial X}{\\partial \\Phi} \\\\ \\frac{\\partial Y}{\\partial R} &amp; \\frac{\\partial X}{\\partial \\Phi} \\end{bmatrix} = \\begin{bmatrix} \\cos{(2\\pi(ft + \\Phi))} &amp; 2\\pi R \\sin{(2\\pi(ft + \\Phi))} \\\\ \\sin{(2\\pi(ft + \\Phi))} &amp; 2\\pi R \\cos{(2\\pi(ft + \\Phi))} \\end{bmatrix}, \\end{gather*} with jacobian \\begin{gather*} -2\\pi R = -2\\pi \\sqrt{X^2 + Y^2} \\end{gather*} and inverse Jacobian \\begin{gather*} \\frac{1}{-2\\pi \\sqrt{X^2 + Y^2}}. \\end{gather*} Furthermore, \\begin{gather*} f(r,\\Phi) = re^{-r^2/2} \\end{gather*} and \\begin{gather*} f(x,y) = \\frac{e^{-(x^2+y^2)/2}\\sqrt{x^2 + y^2}}{2\\pi \\sqrt{x^2 + y^2}} = \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}}\\frac{e^{-y^2/2}}{\\sqrt{2\\pi}} \\quad \\square \\end{gather*} "],
["trends.html", "3 Trends 3.1 Least squares estimation for linear regression trend 3.2 Variance of mean estimator 3.3 Variance of mean estimator #2 3.4 Hours 3.5 Wages 3.6 Beer sales 3.7 Winnebago 3.8 Retail 3.9 Prescriptions 3.10 Hours (revisited) 3.11 Wages (revisisted) 3.12 Beersales (revisited) 3.13 Winnebago (revisited) 3.14 Retail (revisited) 3.15 Prescriptions (revisited) 3.16 Variance estimator for sample mean 3.17 Equation 3.2.6 3.18 Equation 3.2.7", " 3 Trends 3.1 Least squares estimation for linear regression trend We begin by taking the partial derivatives with respect to \\(\\beta_0\\). \\[ \\frac{\\partial}{\\partial{\\beta_0}} \\mathcal{Q}(\\beta_0, \\beta_1) = -2\\sum_{t=1}^n (Y_t - \\beta_0 - \\beta_1 t) \\] We set it to \\(0\\) and from this retrieve \\begin{align*} -2\\sum_{t=1}^n (Y_t - \\beta_0 - \\beta_1 t) = &amp; 0 \\implies \\\\ \\sum_{t=1}^n Y_t - n\\beta_0 - \\beta_1 \\sum_{t=1}^n t = &amp; 0 \\implies \\\\ \\beta_0 = \\frac{\\sum_{t=1}^n Y_t - \\beta_1 \\sum_{t=1}^n t}{n} = &amp; \\bar{Y} - \\beta_1 \\bar{t} \\end{align*} Next, we take the partial derivative with respect to \\(\\beta_1\\); \\[ \\frac{\\partial}{\\partial{\\beta_1}} \\mathcal{Q}(\\beta_0, \\beta_1) = -2\\sum_{t=1}^n t(Y_t - \\beta_0 - \\beta_1 t) \\] Setting this to \\(0\\) as well, multiplying both sides with \\(-1/2\\) and rearranging results in \\begin{align*} -2\\sum_{t=1}^n t (Y_t - \\beta_0 - \\beta_1 t) = &amp; 0 \\implies \\\\ \\beta_1 \\sum_{t=1}^n t^2 = &amp; \\sum_{t=1}^n Y_t t - \\beta_0 \\sum_{t=1}^n t \\end{align*} Then, substituting with the result gained previously for \\(\\beta_0\\), we get \\begin{align*} \\beta_1 \\sum_{t=1}^n t^2 = &amp; \\sum_{t=1}^n Y_t t - \\left( \\frac{\\sum_{t=1}^n Y_t}{n} - \\beta_1 \\frac{\\sum_{t=1}^n}{n} \\right) \\sum_{t=1}^n t \\iff \\\\ \\beta_1 \\left( \\sum_{t=1}^n t^2 - \\frac{(\\sum_{t=1}^n t)^2}{n} \\right) = &amp; \\sum_{t=1}^n Y_t t - \\frac{\\sum_{t=1}^n Y_t \\sum_{t=1}^n t}{n} \\iff \\\\ \\beta_1 = &amp; \\frac{n\\sum_{t=1}^n Y_tt - \\sum_{t=1}^nY_t \\sum_{t=1}^n t}{n \\sum_{t=1}^n t^2 - \\left( \\sum_{t=1}^n t \\right)^2} = \\frac{\\sum_{t=1}^n (Y_t - \\bar{Y})(t-\\bar{t})}{\\sum_{t=1}^n (t-\\bar{t})^2} \\quad \\square \\end{align*} 3.2 Variance of mean estimator \\[ \\bar{Y} = \\frac{1}{n}\\sum_{t=1}^n Y_t = \\frac{1}{n} \\sum_{t=1}^n(\\mu + e_t - e_{t-1}) = \\mu + \\frac{1}{n} \\sum_{t=1}^n (e_t - e_{t-1}) = \\mu + \\frac{1}{n}(e_n - e_0) \\] \\[ \\text{Var}[\\bar{Y}] = \\text{Var}[\\mu + \\frac{1}{n}(e_n - e_0)] = \\frac{1}{n^2}(\\sigma_e^2 + \\sigma_e^2) = \\frac{2\\sigma_e^2}{n^2} \\] It is uncommon for the sample size to have such a large impact on the variance estimator for the sample mean. Setting \\(Y_t = \\mu + e_t\\) instead gives \\[ \\bar{Y} = \\frac{1}{n}\\sum_{t=1}^n Y_t = \\frac{1}{n} \\sum_{t=1}^n(\\mu + e_t) = \\mu + \\frac{1}{n} \\sum_{t=1}^n e_t \\] \\[ \\text{Var}[\\bar{Y}] = \\text{Var} \\left[ \\mu + \\frac{1}{n} \\sum_{t=1}^n e_t \\right] = 0 + \\frac{1}{n^2} \\times n \\sigma_e^2 = \\frac{\\sigma_e^2}{n}. \\] 3.3 Variance of mean estimator #2 \\[ \\bar{Y} = \\frac{1}{n} \\sum_{t=1}^n(\\mu + e_t + e_{t-1}) = \\mu + \\frac{1}{n} \\sum_{t=1}^n (e_t + e_{t-1}) = \\mu + \\frac{1}{n} \\left( e_n + e_0 + 2 \\sum_{t=1}^{n-1} t \\right) \\] \\[ \\text{Var}[\\bar{Y}] = \\frac{1}{n^2}(\\sigma_e^2 + \\sigma_e^2 + 4(n-1) \\sigma_e^2 ) = \\frac{1}{n^2}2(2n-1)\\sigma_e^2 \\] Setting \\(Y_t = \\mu + e_t\\) instead gives the result from 3.2. We note that for large \\(n\\) the variance if approximately four times larger with \\(Y_t = \\mu + e_t + e_{t-1}\\). 3.4 Hours a library(TSA) data(&quot;hours&quot;) xyplot(hours) Figure 3.1: Monthly values of the average hours worked per week in the U.S. manufacturing sector. In Figure 1 we see a steep incline between 83 and 84. There also appears to be a seasonal trend with generally longer work hours later in the year apart from the summer; 1984, however, does not exhibit as clear a pattern. b months &lt;- c(&quot;J&quot;, &quot;A&quot;, &quot;S&quot;, &quot;O&quot;, &quot;N&quot;, &quot;D&quot;, &quot;J&quot;, &quot;F&quot;, &quot;M&quot;, &quot;A&quot;, &quot;M&quot;, &quot;J&quot;) xyplot(hours, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.text(x = x, y = y, labels = months) }) Figure 3.2: Monthly values of average hours worked per week with superposed initials of months. Here, in Figure 2, our interpretation is largely the same. It is clear that December stands out as the month with the longest weekly work hours whilst February and January are low-points, demonstrating a clear trend. 3.5 Wages a data(&quot;wages&quot;) xyplot(wages, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.text(x, y, labels = months) }) Figure 3.3: Monthly average hourly wages for workers in the U.S. apparel and textile industry. There is a positive trend with seasonality: August is a low-point for wages. Generally, there seems to be larger increases in the fall. b wages_fit1 &lt;- lm(wages ~ time(wages)) summary(wages_fit1) ## ## Call: ## lm(formula = wages ~ time(wages)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.2383 -0.0498 0.0194 0.0585 0.1314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.49e+02 1.11e+01 -49.2 &lt;2e-16 *** ## time(wages) 2.81e-01 5.62e-03 50.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.083 on 70 degrees of freedom ## Multiple R-squared: 0.973, Adjusted R-squared: 0.972 ## F-statistic: 2.5e+03 on 1 and 70 DF, p-value: &lt;2e-16 wages_rst &lt;- rstudent(wages_fit1) c xyplot(wages_rst ~ time(wages_rst), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;) Figure 3.4: Residual plot We still seem to have autocorrelation related to the time and not white noise. d wages_fit2 &lt;- lm(wages ~ time(wages) + I(time(wages)^2)) summary(wages_fit2) ## ## Call: ## lm(formula = wages ~ time(wages) + I(time(wages)^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.14832 -0.04144 0.00156 0.05009 0.13984 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.49e+04 1.02e+04 -8.34 4.9e-12 *** ## time(wages) 8.53e+01 1.03e+01 8.31 5.4e-12 *** ## I(time(wages)^2) -2.14e-02 2.59e-03 -8.28 6.1e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.059 on 69 degrees of freedom ## Multiple R-squared: 0.986, Adjusted R-squared: 0.986 ## F-statistic: 2.49e+03 on 2 and 69 DF, p-value: &lt;2e-16 wages_rst2 &lt;- rstudent(wages_fit2) e xyplot(wages_rst2 ~ time(wages_rst), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;) (#fig:wages_quad_resid)Residual plot for our quadratic model. This looks more like random noise but there is still clear autocorrelation between the fitted residuals that we have yet to capture in our model. 3.6 Beer sales a data(beersales) xyplot(beersales) Figure 3.5: Monthly U.S. beer sales. Clear seasonal trends. There is an initial positive trend from 1975 to around 1981 that then levels out. b months &lt;- c(&quot;J&quot;, &quot;F&quot;, &quot;M&quot;, &quot;A&quot;, &quot;M&quot;, &quot;J&quot;, &quot;J&quot;, &quot;A&quot;, &quot;S&quot;, &quot;O&quot;, &quot;N&quot;, &quot;D&quot;) xyplot(beersales, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.text(x, y, labels = months) }) Figure 3.6: Monthly U.S. beer sales annotated with the months’ initials. It is now evident that the peaks are in the warm months and the slump in the winter and fall months. December is a particular low point, while May, June, and July seem to be the high points. c beer_fit1 &lt;- lm(beersales ~ season(beersales)) pander(summary(beer_fit1))   Estimate Std. Error t value Pr(&gt;|t|) season(beersales)February -0.1426 0.3732 -0.382 0.7029 season(beersales)March 2.082 0.3732 5.579 8.771e-08 season(beersales)April 2.398 0.3732 6.424 1.151e-09 season(beersales)May 3.599 0.3732 9.643 5.322e-18 season(beersales)June 3.85 0.3732 10.31 6.813e-20 season(beersales)July 3.769 0.3732 10.1 2.812e-19 season(beersales)August 3.609 0.3732 9.669 4.494e-18 season(beersales)September 1.573 0.3732 4.214 3.964e-05 season(beersales)October 1.254 0.3732 3.361 0.0009484 season(beersales)November -0.04797 0.3732 -0.1285 0.8979 season(beersales)December -0.4231 0.3732 -1.134 0.2585 (Intercept) 12.49 0.2639 47.31 1.786e-103 Fitting linear model: beersales ~ season(beersales) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 192 1.056 0.7103 0.6926 All comparisons are made against january. The model helpfully explains approximately 0.71 of the variance and is statistically significant. Most of the factors are significant (mostly the winter months as expected). d xyplot(rstudent(beer_fit1) ~ time(beersales), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(beersales)), col = 1) }) Figure 3.7: Beer sales residual plot. Looking at the residuals in 3.7 We don’t have a good fit to our data; in particular, wee’re not capturing the long-term trend. e beer_fit2 &lt;- lm(beersales ~ season(beersales) + time(beersales) + I(time(beersales) ^ 2)) pander(summary(beer_fit2))   Estimate Std. Error t value Pr(&gt;|t|) season(beersales)February -0.1579 0.209 -0.7554 0.451 season(beersales)March 2.052 0.209 9.818 1.864e-18 season(beersales)April 2.353 0.209 11.26 1.533e-22 season(beersales)May 3.539 0.209 16.93 6.063e-39 season(beersales)June 3.776 0.209 18.06 4.117e-42 season(beersales)July 3.681 0.209 17.61 7.706e-41 season(beersales)August 3.507 0.2091 16.78 1.698e-38 season(beersales)September 1.458 0.2091 6.972 5.89e-11 season(beersales)October 1.126 0.2091 5.385 2.268e-07 season(beersales)November -0.1894 0.2091 -0.9059 0.3662 season(beersales)December -0.5773 0.2092 -2.76 0.00638 time(beersales) 71.96 8.867 8.115 7.703e-14 I(time(beersales)^2) -0.0181 0.002236 -8.096 8.633e-14 (Intercept) -71498 8791 -8.133 6.932e-14 Fitting linear model: beersales ~ season(beersales) + time(beersales) + I(time(beersales)^2) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 192 0.5911 0.9102 0.9036 This model fits the data better, explaining roughly 0.91 of the variance. f xyplot(rstudent(beer_fit2) ~ time(beersales), type = &quot;l&quot;, xlab = &quot;Time&quot;, yla = &quot;Studentized residuals&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(beersales)), col = 1) }) Figure 3.8: Beer sales residual plot from the quadratic fit. Many of the values are still not being predicted successfully but at least we’re able to model the long term trend better. 3.7 Winnebago a data(winnebago) xyplot(winnebago) Figure 3.9: Monthly unit sales of recreational vehicles from Winnebago. b winn_fit1 &lt;- lm(winnebago ~ time(winnebago)) pander(summary(winn_fit1))   Estimate Std. Error t value Pr(&gt;|t|) time(winnebago) 200.7 17.03 11.79 1.777e-17 (Intercept) -394886 33540 -11.77 1.87e-17 Fitting linear model: winnebago ~ time(winnebago) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 64 209.7 0.6915 0.6865 The model is significant and explains 0.69 of the variance. xyplot(rstudent(winn_fit1) ~ time(winnebago), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;) Figure 3.10: Residuals for the linear fit for the winnebago data. The fit is poor (Figure 3.10. It is not random and it is clear that we’re making worse predictions for later yers. c To produce a better fit, we transform the outcome with the natural logarithm. winn_fit_log &lt;- lm(log(winnebago) ~ time(winnebago)) pander(summary(winn_fit_log))   Estimate Std. Error t value Pr(&gt;|t|) time(winnebago) 0.5031 0.03199 15.73 2.575e-23 (Intercept) -984.9 62.99 -15.64 3.45e-23 Fitting linear model: log(winnebago) ~ time(winnebago) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 64 0.3939 0.7996 0.7964 The model is better, explaining almost 0.8 of the variance. d xyplot(rstudent(winn_fit_log) ~ time(winnebago), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(winnebago)), col = 1) }) Figure 3.11: Residual plot after natural log transformation. This looks more like random noise (Figure 3.11. Values still cling together somewhat but it is certainly better than the linear model. We’re still systematically overpredictinig the values for some months, however. e winn_fit_seasonal &lt;- lm(log(winnebago) ~ season(winnebago) + time(winnebago)) pander(summary(winn_fit_seasonal))   Estimate Std. Error t value Pr(&gt;|t|) season(winnebago)February 0.6244 0.1818 3.434 0.001188 season(winnebago)March 0.6822 0.1909 3.574 0.0007793 season(winnebago)April 0.8096 0.1908 4.243 9.301e-05 season(winnebago)May 0.8695 0.1907 4.559 3.246e-05 season(winnebago)June 0.8631 0.1907 4.526 3.627e-05 season(winnebago)July 0.5539 0.1907 2.905 0.00542 season(winnebago)August 0.5699 0.1907 2.988 0.004305 season(winnebago)September 0.5757 0.1907 3.018 0.00396 season(winnebago)October 0.2635 0.1908 1.381 0.1733 season(winnebago)November 0.2868 0.1819 1.577 0.1209 season(winnebago)December 0.248 0.1818 1.364 0.1785 time(winnebago) 0.5091 0.02571 19.8 1.351e-25 (Intercept) -997.3 50.64 -19.69 1.718e-25 Fitting linear model: log(winnebago) ~ season(winnebago) + time(winnebago) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 64 0.3149 0.8946 0.8699 The fit is improved further. We have a R2 of 0.89 and significance for most of our seasonal means as well as the time trend. f xyplot(rstudent(winn_fit_seasonal) ~ time(winnebago), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, col = 1, pch = as.vector(season(winnebago))) }) This is acceptable even if our residuals are quite large for some of the values, notably at the start of the series. 3.8 Retail a data(retail) xyplot(retail, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(retail)), col = 1) }) Figure 3.12: Total retail sales in the U.K. in billions pounds. Plotting the retail sales trend there seems to be a long-term linear trend as well as heavy seasonality in tht December – and to slighter extent also November and October – exhibit regular surges in retail sales. b retail_lm &lt;- lm(retail ~ season(retail) + time(retail)) pander(summary(retail_lm))   Estimate Std. Error t value Pr(&gt;|t|) season(retail)February -3.015 1.29 -2.337 0.02024 season(retail)March 0.07469 1.29 0.05791 0.9539 season(retail)April 3.447 1.305 2.641 0.008801 season(retail)May 3.108 1.305 2.381 0.01803 season(retail)June 3.074 1.305 2.355 0.01932 season(retail)July 6.053 1.305 4.638 5.757e-06 season(retail)August 3.138 1.305 2.404 0.01695 season(retail)September 3.428 1.305 2.626 0.009187 season(retail)October 8.555 1.305 6.555 3.336e-10 season(retail)November 20.82 1.305 15.95 1.274e-39 season(retail)December 52.54 1.305 40.25 3.169e-109 time(retail) 3.67 0.04369 84 5.206e-181 (Intercept) -7249 87.24 -83.1 6.41e-180 Fitting linear model: retail ~ season(retail) + time(retail) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 255 4.278 0.9767 0.9755 This seems like an effective model, explaining 0.98 of the variance in retail sales. c xyplot(rstudent(retail_lm) ~ time(retail), type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Studentized residuals&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(retail)), col = 1) }) Figure 3.13: Studentized residuals for our seasonality + linear model of retail sales. The residual plot (Figure 3.13) tells a different story: we’re underpredicting values for early period and overpredicting values for the later years – however, this should be an easy fix. 3.9 Prescriptions a data(prescrip) xyplot(prescrip, ylab = &quot;Prescription costs&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(prescrip)), col = 1) }) Figure 3.14: Monthly U.S. prescription costs. Figure 3.14 shows a clear, smooth, and cyclical seasonal trend. Values are genereally higher for the summer months and there seems to be an exponential increase long-term. b pchange &lt;- diff(prescrip) / prescrip xyplot(pchange ~ time(prescrip), type = &quot;l&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(pchange)), col = 1) }) Figure 3.15: Percentage changes from month-to-month in prescription costs. The monthly percentage difference series looks rather stationary. c pres_cos &lt;- lm(pchange ~ harmonic(pchange)) pander(summary(pres_cos))   Estimate Std. Error t value Pr(&gt;|t|) harmonic(pchange)cos(2pit) -0.006605 0.003237 -2.041 0.04542 harmonic(pchange)sin(2pit) 0.01612 0.003208 5.026 4.291e-06 (Intercept) 0.01159 0.002282 5.08 3.508e-06 Fitting linear model: pchange ~ harmonic(pchange) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 67 0.01862 0.3126 0.2912 We explain 0.31 of the variance. The model is significant though. d xyplot(rstudent(pres_cos) ~ time(prescrip), type = &quot;l&quot;) Figure 3.16: Residuals for our cosine model. The residual plot in Figure 3.16 looks rather random. 3.10 Hours (revisited) a data(hours) hours_quad &lt;- lm(hours ~ time(hours) + I(time(hours)^2)) pander(summary(hours_quad))   Estimate Std. Error t value Pr(&gt;|t|) time(hours) 515.9 116.4 4.431 4.314e-05 I(time(hours)^2) -0.1299 0.02933 -4.428 4.353e-05 (Intercept) -512240 115544 -4.433 4.281e-05 Fitting linear model: hours ~ time(hours) + I(time(hours)^2) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 60 0.423 0.5921 0.5778 Both the linear and quadratic trends are significant. We explain 59% of the variance. b xyplot(rstudent(hours_quad) ~ seq_along(hours), type = &quot;l&quot;, xlab = &quot;Index&quot;, ylab = &quot;Studentized residuals&quot;, panel = function(x, y, ...) { panel.xyplot(x, y, ...) panel.xyplot(x, y, pch = as.vector(season(hours)), col = 1) }) Figure 3.17: Studentized residuals for our quadratic fit for the hours series. We’re clearly missing the seasonal trend here. February is underpredicted and December overpredicted, for instance. c We run the Runs test to check for dependence between our observations. runs(rstudent(hours_quad)) ## $pvalue ## [1] 0.00012 ## ## $observed.runs ## [1] 16 ## ## $expected.runs ## [1] 31 ## ## $n1 ## [1] 31 ## ## $n2 ## [1] 29 ## ## $k ## [1] 0 We have more runs than expected and a significant test at \\(p = 0.00012\\), confirming out suspicions from (b). d lat_acf(rstudent(hours_quad)) Figure 3.18: Autocorrelation plot for the hours dataset. Figure 3.18 makes the autocorrelation clear: for the first 5–6 values there is positive correlation, which then seems to reverse for the later values. Some of these are significant. e qqmath(rstudent(hours_quad), asp = 1, xlab = &quot;Theoretical quantities&quot;, ylab = &quot;Studentized residuals&quot;, panel = function(x, ...) { panel.qqmathline(x, ...) panel.qqmath(x, ...) }) densityplot(rstudent(hours_quad), xlab = &quot;Studentized residuals&quot;, ylab = &quot;Density&quot;) Figure 3.19: Normality plots The distribution is somewhat light-tailed but otherwise look quite normal. 3.11 Wages (revisisted) a # data(wages) wages_quad &lt;- lm(wages ~ time(wages) + I(time(wages)^2)) pander(summary(wages_quad))   Estimate Std. Error t value Pr(&gt;|t|) time(wages) 85.34 10.27 8.309 5.439e-12 I(time(wages)^2) -0.02143 0.002588 -8.282 6.104e-12 (Intercept) -84950 10191 -8.336 4.865e-12 Fitting linear model: wages ~ time(wages) + I(time(wages)^2) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 72 0.05889 0.9864 0.986 This quadratic fit explains much of the variance (0.99). b runs(rstudent(wages_quad)) ## $pvalue ## [1] 1.6e-07 ## ## $observed.runs ## [1] 15 ## ## $expected.runs ## [1] 37 ## ## $n1 ## [1] 33 ## ## $n2 ## [1] 39 ## ## $k ## [1] 0 Juding from the output of the Runs test, however, there is evidence to suggest that we have dependence among variables. c lat_acf(rstudent(wages_quad)) (#fig:wages_acf)Autocorrelation for the quadratic fit on the wages time series. However, the autocorrelation plot (Figure @ref(fig:wages_acf)) makes clear that we are dealing with a lot of auttocorrelation and this is obviously because we haven’t accounted for the seasonal trend in the series. d Let’s look at some normality plots as well. figa &lt;- qqmath(rstudent(wages_quad), xlab = &quot;Theoretical quantities&quot;, asp = 1, ylab = &quot;Studentized residuals&quot;, panel = function(x, ...) { panel.qqmathline(x, ...) panel.qqmath(x, ...) }) figb &lt;- densityplot(rstudent(wages_quad), xlab = &quot;Studentized residuals&quot;) gridExtra::grid.arrange(figa, figb, ncol = 2) Figure 3.20: Normality plots for the wages data with a quadratic fit. The normality plots (Figure 3.20) testifies that the distribution of the residuals is somewhat heavy-tailed and ever-so-slightly left-skewed. 3.12 Beersales (revisited) a First, we just collect the residuals. #data(beersales) beer_quad_seasonal &lt;- lm(beersales ~ time(beersales) + I(time(beersales)^2) + season(beersales)) beer_resid &lt;- rstudent(beer_quad_seasonal) b Next, we perform a Runs test. runs(beer_resid) ## $pvalue ## [1] 0.013 ## ## $observed.runs ## [1] 79 ## ## $expected.runs ## [1] 97 ## ## $n1 ## [1] 90 ## ## $n2 ## [1] 102 ## ## $k ## [1] 0 The test is significant (\\(p = 0.01\\)). c lat_acf(beer_resid) Figure 3.21: Autocorrelation for the beer sales model. Correlations are significant for several of the lags, leading us to question independence. d figa &lt;- qqmath(beer_resid, xlab = &quot;Theoretical quantities&quot;, asp = 1, ylab = &quot;Studentized residuals&quot;, panel = function(x, ...) { panel.qqmathline(x, ...) panel.qqmath(x, ...) }) figb &lt;- densityplot(beer_resid, xlab = &quot;Studentized residuals&quot;) gridExtra::grid.arrange(figa, figb, ncol = 2) Figure 3.22: Normality plots for the beersales series after a linear, quadratic and seasonal fit. 3.13 Winnebago (revisited) a winn_resid &lt;- rstudent(winn_fit_seasonal) b runs(winn_resid) ## $pvalue ## [1] 0.00024 ## ## $observed.runs ## [1] 18 ## ## $expected.runs ## [1] 33 ## ## $n1 ## [1] 29 ## ## $n2 ## [1] 35 ## ## $k ## [1] 0 The Runs test is signficant. We have fewer runs than expected. c lat_acf(winn_resid) Figure 3.23: Autcorrelation for the winnebago model. There is evidence of dependence which we have so far not taken into account in the model d figa &lt;- qqmath(winn_resid, xlab = &quot;Theoretical quantities&quot;, asp = 1, ylab = &quot;Studentized residuals&quot;, panel = function(x, ...) { panel.qqmathline(x, ...) panel.qqmath(x, ...) }) figb &lt;- densityplot(winn_resid, xlab = &quot;Studentized residuals&quot;) gridExtra::grid.arrange(figa, figb, ncol = 2) Figure 3.24: Normality plots for the winnebago series fit with a logarithimg and seasonal fit. There is left skew, a large outlier, but otherwise approximate normality. 3.14 Retail (revisited) a retail_lm_seasonal &lt;- lm(retail ~ time(retail) + season(retail)) retail_resid &lt;- rstudent(retail_lm_seasonal) b runs(retail_resid) ## $pvalue ## [1] 9.2e-23 ## ## $observed.runs ## [1] 52 ## ## $expected.runs ## [1] 128 ## ## $n1 ## [1] 136 ## ## $n2 ## [1] 119 ## ## $k ## [1] 0 The Runs test is signficant and we have fewer runs than expected. c lat_acf(retail_resid) Figure 3.25: Autcorrelation for the retail model. There is evidence of dependence which we have so far not taken into account in the model. All of the lags are positive and several are significant too. d figa &lt;- qqmath(retail_resid, xlab = &quot;Theoretical quantities&quot;, asp = 1, ylab = &quot;Studentized residuals&quot;, panel = function(x, ...) { panel.qqmathline(x, ...) panel.qqmath(x, ...) }) figb &lt;- densityplot(retail_resid, xlab = &quot;Studentized residuals&quot;) gridExtra::grid.arrange(figa, figb, ncol = 2) Figure 3.26: Normality plots for the winnebago series fit with a logarithimg and seasonal fit. The distributin of the residuals is considerably light-tailed. 3.15 Prescriptions (revisited) a pres_resid &lt;- rstudent(pres_cos) b runs(pres_resid) ## $pvalue ## [1] 0.0026 ## ## $observed.runs ## [1] 47 ## ## $expected.runs ## [1] 34 ## ## $n1 ## [1] 32 ## ## $n2 ## [1] 35 ## ## $k ## [1] 0 The Runs test is signficant and we have fewer runs than expected. c lat_acf(pres_resid) Figure 3.27: Autcorrelation for the prescriptions model. Some of the lags have correlations that surpass statistical significane. There may be some alternating trends that we have not taken into account. d figa &lt;- qqmath(pres_resid, xlab = &quot;Theoretical quantities&quot;, asp = 1, ylab = &quot;Studentized residuals&quot;, panel = function(x, ...) { panel.qqmathline(x, ...) panel.qqmath(x, ...) }) figb &lt;- densityplot(pres_resid, xlab = &quot;Studentized residuals&quot;) gridExtra::grid.arrange(figa, figb, ncol = 2) Figure 3.28: Normality plots for the prescitions series fit with a cosine model. The distribution of the residuals is somewhat heavy-tailed and left-skewed. 3.16 Variance estimator for sample mean a We have the variance estimator \\[ \\begin{align} \\text{Var}[\\bar{Y}] = &amp; \\frac{\\gamma_0}{n} \\left( 1 + 2\\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\phi^k \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( 1 + 2\\sum_{k=0}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\phi^k - 2\\sum_{k=0}\\left(1 - \\frac{k}{n}\\right)\\phi^k \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( 1 + 2\\sum_{k=0}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\phi^k - 2 \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( -1 + 2\\sum_{k=0}^{n-1}\\phi^k - \\frac{2}{n}\\sum_{k=0}^{n-1}k\\phi^k \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( -1 + 2 \\frac{1-\\phi^n}{1-\\phi} - \\frac{2\\phi}{n} \\sum_{k=0}^{n-1}k\\phi^{k-1} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( -1 + 2 \\frac{1-\\phi^n}{1-\\phi} - \\frac{2\\phi}{n} \\frac{\\partial}{\\partial{\\phi}} \\sum_{k=0}^{n-1}\\phi^k \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( -1 + 2 \\frac{1-\\phi^n}{1-\\phi} - \\frac{2\\phi}{n} \\frac{(1-\\phi)(-n\\phi^{n-1}) - (1-\\phi^n)(-1)}{(1-\\phi)^2} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( -1 + 2 \\frac{1-\\phi^n}{1-\\phi} - \\frac{2\\phi}{n} \\frac{(1-\\phi)(-n\\phi^{n-1}) - (1-\\phi^n)(-1)}{(1-\\phi)^2} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( -1 + 2 \\frac{1-\\phi^n}{1-\\phi} - \\frac{2\\phi}{n} \\frac{1-\\phi^n}{(1-\\phi)^2} + \\frac{2\\phi^n}{1-\\phi} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( \\frac{2-2\\phi^n+2\\phi^n-1+\\phi}{1-\\phi} - \\frac{2\\phi}{n} \\frac{1-\\phi^n}{(1-\\phi)^2} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( \\frac{1+\\phi}{1-\\phi} - \\frac{2\\phi}{n} \\frac{1-\\phi^n}{(1-\\phi)^2} \\right) \\quad \\square \\end{align} \\] b \\[ \\lim_{n \\rightarrow \\infty}\\text{Var}[\\bar{Y}] = \\frac{\\gamma_0}{n}\\left( \\frac{1+\\phi}{1-\\phi} - 0 \\right) = \\frac{\\gamma_0}{n}\\left( \\frac{1+\\phi}{1-\\phi}\\right) \\] since \\(\\phi \\in [-1,1]\\). c phi &lt;- seq(-1, 1, 0.01) var_ybar &lt;- (1 + phi) / (1 - phi) xyplot(var_ybar ~ phi, ylab = expression(Var(bar(Y))), xlab = expression(phi), type = &quot;l&quot;) Figure 3.29: Variance estimation for different values of \\(\\phi\\). Plotting \\(\\text{Var}[\\bar{Y}]\\) for values of \\(\\phi\\) in \\([-1, 1]\\). Shows that variance increases exponentially as \\(\\phi\\) approaches 1, in which case our estimates of \\(\\bar{Y}\\) become increasingly uncertain. 3.17 Equation 3.2.6 \\[ \\begin{align} \\text{Var}[\\bar{Y}] = &amp; \\frac{\\gamma_0}{n} \\sum_{k= -\\infty}^\\infty \\rho_k \\quad \\text{when} \\quad \\rho_k = \\phi^{|k|} \\implies \\\\ = &amp; \\frac{\\gamma_0}{n} \\sum_{k= -\\infty}^\\infty \\phi^{|k|} \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( \\sum_{k = 0}^\\infty \\phi^k + \\sum_{0}^\\infty \\phi^{-k} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\left( \\frac{1}{1-\\phi} - \\frac{1}{1-\\frac{1}{\\phi}} \\right) \\\\ = &amp; \\frac{\\gamma_0}{n} \\frac{1+\\phi}{1-\\phi} \\tag*{$\\square$} \\end{align} \\] 3.18 Equation 3.2.7 \\[ \\begin{gather} \\text{Var}[\\bar{Y}] = \\frac{1}{n^2} \\text{Var}\\left[ \\sum_{i=1}^n Y_i \\right] = \\text{Var}\\left[ \\sum_{i=1}^n \\sum_{j=1}^i e_j \\right] = \\\\ \\frac{1}{n^2}\\text{Var}[e_1 + 2e_2 + 3e_3 + \\dots + ne_n] = \\frac{\\sigma_e^2}{n}\\sum_{k=1}^n k^2 = \\\\ \\frac{\\sigma_e^2}{n} \\frac{n(n+1)(2n + 1)}{6} = \\sigma_e^2 \\frac{(n+1)(2n + 1)}{6} \\tag*{$\\square$} \\end{gather} \\] "],
["models-for-stationary-time-series.html", "4 Models for stationary time series 4.1 First principles 4.2 Sketch autocorrelations 4.3 Max and min correlations for MA(1) 4.4 Non-uniqueness of MA(1) 4.5 Sketch more autocorrelations 4.6 Difference function for AR(1) 4.7 Characteristics of several models 4.8 AR(2) 4.9 Sketching AR(2) processes 4.10 Sketch ARMA(1,1) models 4.11 ARMA(1,2) 4.12 Two MA(2) processes 4.13 Autocorrelation in MA(1) 4.14 Zero-mean stationary process 4.15 Stationarity prerequisite of AR(1) 4.16 Nonstationary AR(1) process 4.17 Solution to AR(1) 4.18 Stationary AR(1) 4.19 Find a simpler model (1) 4.20 Find a simpler model (2) 4.21 ARMA in disguise 4.22 Equivalence of statements 4.23 Covariance of AR(1) 4.24 Recursion 4.25 Final exercise", " 4 Models for stationary time series 4.1 First principles We have the process \\[ Y_t = 5 + e_t - \\frac{1}{2}e_{t-i} + \\frac{1}{4}e_{t-2} \\] and begin by working out its variance \\[ \\begin{aligned} \\text{Var}(Y_t) &amp; = \\text{Var}(5 + e_t - \\frac{1}{2}e_{t-i} + \\frac{1}{4}e_{t-2})\\\\ &amp; = \\text{Var}(e_t) + \\frac{1}{4}\\text{Var}(e_t) + \\frac{1}{16}\\text{Var}(e_t)\\\\ &amp; = \\frac{21}{16}\\sigma_e^2 \\end{aligned} \\] and then the autocovariance at lag 1 \\[ \\begin{aligned} \\text{Cov}(Y_t, Y_{t-1}) &amp; = \\text{Cov}(5+e_t-\\frac{1}{2}e_{t-1}+\\frac{1}{4}e_{t-2}, 5+e_{t-1}-\\frac{1}{2}e_{t-2}+\\frac{1}{4}e_{t-3})\\\\ &amp; = \\text{Cov}(-\\frac{1}{2}e_{t-1},e_{t-1}) + \\text{Cov}(\\frac{1}{4}e_{t-2},-\\frac{1}{2}e_{t-2}) \\\\ &amp; = -\\frac{1}{2}\\text{Var}(e_{t-1}) -\\frac{1}{8}\\text{Var}(e_{t-2})\\\\ &amp; = -\\frac{5}{8}\\sigma_e^2 \\end{aligned} \\] lag 2 \\[ \\begin{aligned} \\text{Cov}(Y_t, Y_{t-2}) &amp; = \\text{Cov}(5+e_t-\\frac{1}{2}e_{t-1}+\\frac{1}{4}e_{t-2}, 5+e_{t-2}-\\frac{1}{2}e_{t-3}+\\frac{1}{4}e_{t-4})\\\\ &amp; = \\frac{1}{4}\\text{Var}(e_{t-2}) \\\\ &amp; = \\frac{1}{4}\\sigma_e^2 \\end{aligned} \\] and lag 3 \\[ \\text{Cov}(Y_t, Y_{t-2}) = \\text{Cov}(5+e_t-\\frac{1}{2}e_{t-1}+\\frac{1}{4}e_{t-2}, 5+e_{t-2}-\\frac{1}{2}e_{t-3}+\\frac{1}{4}e_{t-4}) = 0 \\] which results in the autocorrelation \\[ \\rho_k = \\begin{cases} 1 &amp; k = 0\\\\ \\frac{-\\frac{5}{8}\\sigma_e^2}{\\frac{21}{16}\\sigma_e^2}=-\\frac{10}{21} &amp; k = 1\\\\ \\frac{\\frac{1}{4}\\sigma_e^2}{\\frac{21}{16}\\sigma_e^2}=\\frac{4}{21} &amp; k = 2\\\\ 0 &amp; k = 3\\\\ \\end{cases} \\tag*{$\\square$} \\] 4.2 Sketch autocorrelations a tacf(ma = list(-0.5, -0.4)) Figure 4.1: Autocorrelation with \\(\\theta_1 = 0.5\\) and \\(\\theta_2 = 0.4\\) b tacf(ma = list(-1.2, 0.7)) Figure 4.2: Autocorrelation with \\(\\theta_1 = 1.2\\) and \\(\\theta_2 = -0.7\\) c tacf(ma = list(1, 0.6)) Figure 4.3: Autocorrelation with \\(\\theta_1 = -1\\) and \\(\\theta_2 = -0.6\\) 4.3 Max and min correlations for MA(1) For \\[ \\rho_1 = \\frac{-\\theta}{1+\\theta^2} \\] we retrieve extreme values at \\[ \\frac{\\partial}{\\partial \\theta}\\rho_1 = \\frac{-1(1+\\theta^2)-2\\theta(-\\theta)}{(1+\\theta^2)^2} = \\frac{\\theta^2 - 1}{(1+\\theta^2)^2} = 0 \\] when \\(t = \\begin{cases}-1\\\\1\\end{cases}\\), which gives us \\[ \\begin{aligned} \\max \\rho_1 &amp; = \\frac{-1(-1)}{1+(-1)^2} = 0.5\\\\ \\min \\rho_1 &amp; = \\frac{-1}{1+1^2} = -0.5 \\end{aligned} \\] which we graph in figure 4.4 theta &lt;- seq(-10, 10, by = 0.01) p1 &lt;- (-theta) / (1 + theta^2) plot(theta, p1, type = &quot;l&quot;) points(theta[which.max(p1)], max(p1)) points(theta[which.min(p1)], min(p1)) Figure 4.4: Autocorrelation at lag one for MA(1) with max and min annotated. 4.4 Non-uniqueness of MA(1) \\[ \\frac{-\\frac{1}{\\theta}}{1 + \\left( \\frac{1}{\\theta}\\right)^2} = \\frac{-\\frac{1}{\\theta}\\times\\theta^2}{\\left( 1 + \\frac{1}{\\theta^2} \\right) \\theta^2} = \\frac{-\\theta}{1+\\theta^2} \\tag*{$\\square$} \\] 4.5 Sketch more autocorrelations theta &lt;- c(0.6, -0.6, 0.95, 0.3) lag &lt;- c(10, 10, 20, 10) for (i in seq_along(theta)) { print(tacf(ar = theta[i], lag.max = lag[i])) } Figure 4.5: ACF for various AR(1) processes. 4.6 Difference function for AR(1) a \\[ \\begin{aligned} \\text{Cov}(\\triangledown Y_t, \\triangledown Y_{t-k}) &amp; = \\text{Cov}(Y_t-Y_{t-1}, Y{t-k}-Y_{t-k-1})\\\\ &amp; = \\text{Cov}(Y_t, Y_{t-k}) - \\text{Cov}(Y_{t-1},Y_{t-k}) - \\text{Cov}(Y_t, Y_{t-k-1}) + \\text{Cov}(Y_{t-1}, Y_{t-k-1})\\\\ &amp; = \\frac{\\sigma_e^2}{1-\\phi^2}(\\phi^2 - \\phi^{k-1}-\\phi^{k+1}+\\phi^k) \\\\ &amp; = \\frac{\\sigma_e^2}{1-\\phi^2}\\phi^{k-1}(2\\phi-\\phi2-1)\\\\ &amp; = - \\frac{\\sigma_e^2}{1-\\phi^2}(1-\\phi)^2\\phi^{k-1}\\\\ &amp; = - \\sigma_e^2 \\frac{(1-\\phi)^2}{(1-\\phi)(1+\\phi)}\\\\ &amp; = -\\sigma_e^2 \\frac{1-\\phi}{1+\\phi}\\phi^{k-1} \\end{aligned} \\] as required. b \\[ \\begin{aligned} \\text{Var}(W_t) &amp; = \\text{Var}(Y_t-Y_{t-1})\\\\ &amp; = \\text{Var}(\\phi_1Y_{t-1}+e_t-Y_{t-1})\\\\ &amp; = \\text{Var}(Y_{t-1}(\\phi-1)+\\sigma_e^2)\\\\ &amp; = (\\phi-1)^2\\text{Var}(Y_{t-1}) + \\text{Var}(e_t)\\\\ &amp; = \\frac{\\sigma_e^2}{1-\\phi^2}(\\phi^2-2\\phi+1) + \\sigma_e^2\\\\ &amp; = \\frac{\\sigma_e^2(\\phi^2-2\\phi+1+1-\\phi^2)}{1-\\phi^2}\\\\ &amp; = \\frac{2\\sigma_e^2(1-\\phi)}{1-\\phi^2} \\\\ &amp; = \\frac{2\\sigma_e^2}{1+\\phi} \\tag*{$\\square$} \\end{aligned} \\] 4.7 Characteristics of several models a Only correlation at lag 1. b Only autocorrelation at lag 1 and 2. Shape of process depends on values of coefficients. c Exponentially decaying correlation from lag 0. d Different patterns in ACF that depends on whether roots are complex or real. e Exponentially decaying correlations from lag 1. 4.8 AR(2) First, we have variance \\[ \\text{Var}(Y_t) = \\text{Var}(\\phi_2 Y_{t-2} + e_t) = \\phi_2^2 \\text{Var}Y_{t-2} + \\sigma_e^2 \\] which, assuming stationarity, is equivalent to \\[ \\text{Var}(Y_{t-2}) = \\phi_2^2\\text{Var}(Y_{t-2}) + \\sigma_e^2 \\iff \\\\ \\sigma_e^2 = (1-\\phi_2^2) \\text{Var}(Y_{t-2}) \\iff \\\\ \\text{Var}(Y_{t-2}) = \\frac{\\sigma_e^2}{1-\\phi^2_2} \\] which requires that \\(-1 &lt; \\phi_2 &lt; 1\\) since \\(\\text{Var}(Y_{t-2}) \\geq 0\\). 4.9 Sketching AR(2) processes a \\[ \\begin{split} \\rho_1 &amp; = 0.6\\rho_0 + 0.3\\rho{-1} = 0.6 + 0.3\\rho_1 = 0.8571 \\\\ \\rho_2 &amp; = 0.6\\rho_1+0.3\\rho_0 = 0.81426\\\\ \\rho_3 &amp; = 0.6\\rho_2 + 0.3\\rho_1 = 0.7457 \\end{split} \\] The roots to the characteristic equation are given by \\[ \\frac{\\phi_1 \\pm \\sqrt{\\phi_1^2 + 4\\phi_2}}{-2\\phi_2} = \\frac{0.6 \\pm \\sqrt{0.6 + 4 \\times 0.3}}{-2 \\times 0.3} = -1 \\pm 2.0817 = \\{1.0817, -3.0817\\}. \\] Since both of these roots exceed 1 in absolute value, they are real. Next, we sketch the theoretical autocorrelation function (4.6). tacf(ar = c(0.6, 0.3)) Figure 4.6: ACF for AR(2) with \\(\\phi_1 = 0.6, \\phi_2 = 0.3\\). b Next we write a function to do the work for us. ar2solver &lt;- function(phi1, phi2) { roots &lt;- polyroot(c(1, -phi1, -phi2)) cat(&quot;Roots:\\t\\t&quot;, roots, &quot;\\n&quot;) if (any(Im(roots) &gt; sqrt(.Machine$double.eps))) { damp &lt;- sqrt(-phi2) freq &lt;- acos(phi1 / (2 * damp)) cat(&quot;Dampening:\\t&quot;, damp, &quot;\\n&quot;) cat(&quot;Frequency:\\t&quot;, freq, &quot;\\n&quot;) } tacf(ar = c(phi1, phi2)) } ar2solver(-0.4, 0.5) ## Roots: -1.1+0i 1.9-0i Figure 4.7: ACF for AR(2) with \\(\\phi_1 = -0.4, \\phi_2 = 0.5\\). c ar2solver(1.2, -0.7) ## Roots: 0.86+0.83i 0.86-0.83i ## Dampening: 0.84 ## Frequency: 0.77 Figure 4.8: ACF for AR(2) with \\(\\phi_1 = 1.2, \\phi_2 = -0.7\\). d ar2solver(-1, -0.6) ## Roots: -0.83+0.99i -0.83-0.99i ## Dampening: 0.77 ## Frequency: 2.3 Figure 4.9: ACF for AR(2) with \\(\\phi_1 = -1, \\phi_2 = -0.6\\). e ar2solver(0.5, -0.9) ## Roots: 0.3+1i 0.3-1i ## Dampening: 0.95 ## Frequency: 1.3 Figure 4.10: ACF for AR(2) with \\(\\phi_1 = 0.5, \\phi_2 = -0.9\\). f ar2solver(-0.5, -0.6) ## Roots: -0.4+1.2i -0.4-1.2i ## Dampening: 0.77 ## Frequency: 1.9 Figure 4.11: ACF for AR(2) with \\(\\phi_1 = -0.5, \\phi_2 = -0.6\\). 4.10 Sketch ARMA(1,1) models a tacf(ar = 0.7, ma = -0.4) Figure 4.12: ACF for ARMA(1,1) with \\(\\phi = 0.7\\) and \\(\\theta = 0.4\\). b tacf(ar = 0.7, ma = 0.4) Figure 4.13: ACF for ARMA(1,1) with \\(\\phi = 0.7\\) and \\(\\theta = -0.4\\). 4.11 ARMA(1,2) a \\[ \\begin{aligned} \\text{Cov}(Y_t, Y_{t-k}) &amp; = \\text{E}[(0.8Y_{t-1}+e_t+0.7e_{t-1}+0.6e_{t-2})Y_{t-k}] - \\text{E}(Y_t)\\text{E}(Y_{t-k})\\\\ &amp; = \\text{E}(0.8Y_{t-1}Y_{t-k} + Y_{t-k}e_t + 0.7e_{t-1}Y_{t-k}+0.6e_{t-2}Y_{t-k}) - 0\\\\ &amp; = 0.8\\text{E}(Y_{t-1}Y_{t-k}) + \\text{E}(Y_{t-k}e_t) + 0.7\\text{E}(e_{t-1}Y_{t-k}) + 0.7\\text{E}(e_{t-2}Y_{t-k})\\\\ &amp; = 0.8\\text{E}(Y_{t-1}Y_{t-k})\\\\ &amp; = 0.8\\text{Cov}(Y_t,Y_{t-k+1})\\\\ &amp; = 0.8\\gamma_{k-1} &amp; \\square \\end{aligned} \\] b \\[ \\begin{split} \\text{Cov}(Y_t,Y_{t-2}) &amp; = \\text{E}[0.8Y_{t-1}+e_t+0.7e_{t-1}+0.6e_{t-2})Y_{t-2}]\\\\ &amp; = \\text{E}[(0.8Y_{t-1}+0.6e_{t-2})Y_{t-2}]\\\\ &amp; = 0.8\\text{Cov}(Y_{t-1},Y_{t-2})+0.6\\text{E}(e_{t-2}Y_{t-2})\\\\ &amp; = 0.8\\gamma_1 + 0.6\\text{E}(e_t Y_t)\\\\ &amp; = 0.8\\gamma_1 + 0.6\\text{E}[e_t(0.8Y_{t-1}+e_t+0.7e_{t-1}+0.6e_{t-2})]\\\\ &amp; = 0.8\\gamma_1 + 0.6\\sigma_e^2 \\iff \\\\ \\rho_2 &amp; = 0.8\\rho_1+0.6\\sigma_e^2/\\gamma_0 &amp; \\square \\end{split} \\] 4.12 Two MA(2) processes a For \\(\\theta_1 = \\theta_2 = 1/6\\) we have \\[ \\rho_k = \\frac{-\\frac{1}{6}+\\frac{1}{6}\\times\\frac{1}{6}}{1 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{6}\\right)^2} = \\frac{\\frac{1}{6}\\left(\\frac{1}{6}-1\\right)}{1 + \\frac{2}{36}} = - \\frac{5}{38}. \\] For \\(\\theta_1 = -1\\) and \\(\\theta_2 = 6\\), \\[ \\rho_k = \\frac{1-6}{1+1^2+36} = - \\frac{5}{38} \\tag*{$\\square$}. \\] b For \\(\\theta_1 = \\theta_2 = 1/6\\) we have roots given by \\[ \\frac{\\frac{1}{6} \\pm \\sqrt{\\frac{1}{36}+ 4 \\times \\frac{1}{6}}}{-2\\times \\frac{1}{6}} = - \\frac{1}{2} \\pm \\frac{\\sqrt{\\frac{25}{36}}}{-\\frac{1}{3}} = - \\frac{1}{2} \\pm \\frac{\\frac{5}{6}}{\\frac{1}{3}} = \\{-3, -2\\} \\] and for \\(\\theta_1 = -1\\) and \\(\\theta_2 = 6\\), \\[ \\frac{-1 \\pm \\sqrt{1 + 4 \\times 6}}{-2\\times6} = \\frac{-1\\pm 5}{-12} = \\frac{1}{12} \\pm \\frac{5}{12} = \\left\\{-\\frac{1}{3}, \\frac{1}{2}\\right\\} \\] 4.13 Autocorrelation in MA(1) \\[ \\begin{aligned} &amp; \\text{Var}(Y_{n+1}+Y_n+Y_{n-1}+ \\dots + Y_1) = \\left((n+1)+2n\\rho_1\\right)\\gamma_0 = \\left(1+n(1+2\\rho_1)\\right)\\gamma_0\\\\ &amp; \\text{Var}(Y_{n+1}-Y_n+Y_{n-1}- \\dots + Y_1) = \\left((n+1)-2n\\rho_1 \\right)\\gamma_0 = \\left(1 + n(1-2\\rho_1)\\right)\\gamma_0 \\end{aligned} \\] [ \\begin{cases} \\left(1+n(1+2\\rho-1)\\right) \\geq 0 \\\\ \\left(1+n(1-2\\rho-1)\\right) \\geq 0 \\\\ \\end{cases} \\begin{cases} 1+n+2\\rho_1n \\geq 0 \\\\ 1+n-2\\rho_1n \\geq 0 \\\\ \\end{cases} \\begin{cases} \\rho_1 \\geq \\frac{-(n+1)}{2n}\\\\ \\rho_1 \\leq \\frac{n+1}{2n} \\end{cases} \\begin{cases} \\rho_1 \\geq -\\frac{1}{2}(1+\\frac{1}{n})\\\\ \\rho_1 \\leq \\frac{1}{2}(1+\\frac{1}{n}) \\end{cases} ] where \\(\\rho_1 \\geq |1/2|\\) for all \\(n\\). 4.14 Zero-mean stationary process We set \\(Y_t=e_t−θe_t−1\\) and then we have \\[ \\begin{split} e_t &amp; = \\sum_{j=0}^\\infty \\theta^j Y_{t-j}\\quad \\text{and expanding into} \\\\ &amp; = \\sum_{j=1}^\\infty \\theta^j Y_{t-j} + \\theta^0 Y_{t-0} \\\\ &amp; \\iff \\\\ Y_t &amp; = e_t - \\sum_{j=1}^\\infty \\theta^j Y_{t-j} \\end{split} \\] which is equivalent to \\[ Y_t = \\mu_0 + (1 + \\theta B + \\theta^2 B^2 + \\dots + \\theta^n B^n)e_t \\] which is the definition of a MA(1) process where \\(B\\) is the backshift operator such that \\(Y_t B^k =Y_{t-k}\\). 4.15 Stationarity prerequisite of AR(1) \\[ \\begin{split} \\text{Var}(Y_t) &amp; = \\text{Var}(\\phi Y_{t-1}+e_t) = \\phi^2\\text{Var}(Y_{t-1})+\\sigma_e^2\\\\ &amp; = \\phi^2 \\text{Var}(\\phi Y_{t-2}+e_t)+\\sigma_e^2\\\\ &amp; = \\phi^4 \\text{Var}(Y_{t-2})+2\\sigma_e^2\\\\ &amp; = \\phi^{2n}\\text{Var}(Y_{t-n})+n\\sigma_e^2 \\end{split} \\] And \\(\\lim_{n \\rightarrow \\infty} \\text{Var}(Y_t) = \\infty\\) if \\(|\\phi\\)=1$, which is impossible. 4.16 Nonstationary AR(1) process a \\[ \\begin{split} Y_t &amp; = \\phi Y_{t-1}+e_t \\implies \\\\ - \\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t+j} &amp; = 3 \\left(-\\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t-1+j}\\right) + e_t\\\\ - \\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^{j+1} e_{t+j} &amp; = -\\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t-1+j} + \\frac{1}{3} e_t\\\\ - \\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^{j+1} e_{t+j} &amp; = -\\sum_{j=2}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t-1+j}\\\\ - \\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^{j+1} e_{t+j} &amp; = -\\sum_{j+1=2}^\\infty \\left(\\frac{1}{3}\\right)^{j+1} e_{t+j} &amp; \\square \\end{split} \\] b \\[ \\text{E}(Y_t) = \\text{E}(\\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t+j}) = 0 \\] since all terms are uncorrelated white noise. \\[ \\begin{gathered} \\text{Cov}(Y_t,Y_{t-1}) = \\text{Cov}\\left( -\\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t+j}, \\sum_{j=1}^\\infty \\left(\\frac{1}{3}\\right)^j e_{t+j-1} \\right) = \\\\ \\text{Cov}\\left(-\\frac{1}{3}e_{t+1} - \\left(\\frac{1}{3}\\right)^2e_{t+2} - \\dots - \\left(\\frac{1}{3}\\right)^n e_{t+n}, -\\frac{1}{3}e_{t} - \\left(\\frac{1}{3}\\right)^2e_{t+1} - \\dots - \\left(\\frac{1}{3}\\right)^n e_{t+n-1} \\right) = \\\\ \\text{Cov}\\left(-\\frac{1}{3}e_{t+1},-\\frac{1}{3^2}e_{t+1}\\right) + \\text{Cov}\\left(-\\frac{1}{3}e_{t+2},-\\frac{1}{3^3}e_{t+2}\\right) + \\dots + \\text{Cov}\\left(-\\frac{1}{3}e_{t+n},-\\frac{1}{3^{n+1}}e_{t+n}\\right) = \\\\ \\frac{1}{26}\\sigma_e^2\\left(1 + \\frac{1}{3} + \\frac{1}{3^2} + \\dots + \\frac{1}{3^n} \\right) \\end{gathered} \\] which is free of \\(t\\). c It is unsatisfactory because Y_t depends on future observations. 4.17 Solution to AR(1) a \\[ \\frac{1}{2}\\left(10\\left(\\frac{1}{2}\\right)^{t-1} + e_{t-1} + \\frac{1}{2}e_{t-2} + \\left(\\frac{1}{2}\\right)^2e_{t-3} + \\dots + \\left(\\frac{1}{2}\\right)^{n-1}e_{t-n}\\right) + e_{t-1} = \\\\ 10 \\left(\\frac{1}{2}\\right)^t + \\frac{1}{2}e_{t-1} + \\left(\\frac{1}{2}\\right)^2e_{t-2} + \\left(\\frac{1}{2}\\right)^3 e_{t-3} + \\dots + \\left( \\frac{1}{2}\\right)^n e_{t-n} + e_{t-1} = \\\\ 10\\left(\\frac{1}{2}\\right)^{t-1} + e_{t-1} + \\frac{1}{2}e_{t-2} + \\left(\\frac{1}{2}\\right)^2e_{t-3} + \\dots + \\left(\\frac{1}{2}\\right)^{n-1}e_{t-n} \\qquad \\square \\] b \\(\\text{E}(Y_t) = 10 \\left( \\frac{1}{2}\\right)^t\\) varies with \\(t\\) and thus is not stationary. 4.18 Stationary AR(1) a \\[ \\text{E}(W_t) = \\text{E}(Y_t + c\\phi^t) = \\text{E}(Y_t) + \\text{E}(c\\phi^t) = 0 + c\\phi^t = c\\phi^t \\tag*{$\\square$} \\] b \\[ \\phi(Y_{t-1}+c\\phi^{t-1})+e_t = \\phi Y_{t-1} + c\\phi^t + e_t = \\phi \\left( \\frac{Y_t - e_t}{\\phi}\\right) + c \\phi^t + e_t = Y_t + c\\phi^t \\tag*{$\\square$} \\] c No, (W_t) is not free of \\(t\\). 4.19 Find a simpler model (1) This is similar to an AR(1) with \\(\\rho_k = -(-0.5)^k\\). ARMAacf(ar = -0.5, lag.max = 7) ## 0 1 2 3 4 5 6 7 ## 1.0000 -0.5000 0.2500 -0.1250 0.0625 -0.0312 0.0156 -0.0078 ARMAacf(ma = -c(0.5, -0.25, 0.125, -0.0625, 0.03125, -0.0015625)) ## 0 1 2 3 4 5 6 7 ## 1.0000 -0.4997 0.2492 -0.1232 0.0589 -0.0240 0.0012 0.0000 4.20 Find a simpler model (2) This is like an ARMA(1,1) with \\(\\phi = -0.5\\) and \\(\\theta = 0.5\\). [ \\begin{cases} \\psi_1 = \\phi - \\theta = 1\\\\ \\psi_2 = (\\phi - \\theta)\\phi = -0.5 \\end{cases} \\begin{cases} \\theta = 0.5\\\\ \\phi = -0.5 \\end{cases} ] ARMAacf(ma = -c(1, -0.5, 0.25, -0.125, 0.0625, -0.03125, 0.015625)) ## 0 1 2 3 4 5 6 7 8 ## 1.0000 -0.7142 0.3570 -0.1783 0.0887 -0.0435 0.0201 -0.0067 0.0000 ARMAacf(ar = -0.5, ma = -0.5, lag.max = 8) ## 0 1 2 3 4 5 6 7 8 ## 1.0000 -0.7143 0.3571 -0.1786 0.0893 -0.0446 0.0223 -0.0112 0.0056 4.21 ARMA in disguise a \\[ \\begin{gathered} \\text{Cov}(Y_t, Y_{t-k}) = \\text{Cov}(e_{t-1}-e_{t-2}+0.5e_{t-3}, e_{t-1-k}-e_{t-2-k}+0.5e_{t-3-k}) = \\\\ \\gamma_k = \\begin{cases} \\sigma_e^2 + \\sigma_e^2 + 0.25\\sigma_e^2 = 2.25\\sigma_e^2 &amp; k = 0\\\\ -\\sigma_e^2-0.5\\sigma_e^2=-1.5\\sigma_e^2 &amp; k = 1\\\\ 0.5\\sigma_e^2 &amp; k = 2 \\end{cases} \\end{gathered} \\] b This is an ARMA(p,q) in the sense that \\(p = 0\\) and \\(q = 2\\), that is, it is in fact an MA(2) process \\(Y_t = e_t - e_{t-1}+0.5e_{t-2}\\) with \\(\\theta_1 = 1, \\theta2 = -0.5\\). 4.22 Equivalence of statements \\[ 1 - \\phi_1x - \\phi_2x^2 - \\dots - \\phi_p x^p \\implies x^k \\left( \\left(\\frac{1}{k}\\right)^p - \\phi_1 \\left(\\frac{1}{k}\\right)^{p-1} \\dots - \\phi_p \\right) \\] Thus if \\(x_1 = G\\) is a root to the above, \\(\\frac{1}{x_1} = \\frac{1}{G}\\) must be a root to \\[ x^p - \\phi_1 x^{p-1} -\\phi_2 x^{p-2} - \\dots -\\phi_p \\] 4.23 Covariance of AR(1) a \\[ \\begin{gathered} \\text{Cov}(Y_t - \\phi Y_{t+1}, Y_{t-k} - \\phi Y_{t-k+1}) = \\\\ \\text{Cov}(Y_t, Y_{t-k}) - \\phi \\text{Cov}(Y_t, Y_{t+1-k}) - \\phi\\text{Cov}(Y_{t+1},Y_{t-k}) + \\phi^2\\text{Cov}(Y_{t+1, Y_{t+1-k}}) = \\\\ \\frac{\\sigma_e^2}{1-\\phi^2}\\left(\\phi^k - \\phi \\phi^{k-1} - \\phi \\phi^{k+1} + \\phi^2 \\phi\\right) =\\\\ \\frac{\\sigma_e^2}{1-\\phi^2}\\left( \\phi^k - \\phi^k - \\phi^{k+2} + \\phi^{k+2} \\right) = 0 \\end{gathered} \\] b First, \\[ Y_{t+k} = \\phi Y_{t+k-1} + e_t + k \\implies \\] \\[ \\begin{split} \\text{Cov}(b_t, Y_{t+k}) &amp; = \\text{Cov}(Y_t - \\phi Y_{t+1, Y_{t+n}})\\\\ &amp; = \\text{Cov}(Y_t,Y_{t+k}) - \\phi \\text{Cov}(Y_{t+1}, Y_{t+k)})\\\\ &amp; = \\frac{\\sigma_e^2}{1-\\phi^2}\\phi^k - \\phi \\frac{\\sigma_e^2}{1-\\phi^2}\\phi^{k-1}\\\\ &amp; = 0 &amp; \\square \\end{split} \\] 4.24 Recursion a \\[ \\begin{split} E(Y_0) &amp; = E[c_1e_0] = c_1E[e_0] = 0\\\\ E(Y_1) &amp; = E(c_2Y_0 + e_1) = c_2 E(Y_0) = 0\\\\ E(Y_2) &amp; = E(\\phi_1 Y_1 + \\phi_2 Y_0 + e_t) = 0\\\\ &amp; \\vdots \\\\ E(Y_t) &amp; = E(\\phi_1 Y_{t-1} + \\phi_2 Y_{t-2}) = 0 \\end{split} \\] b We have \\[ \\begin{cases} \\text{Var}(Y_0) = c_1^2\\text{Var}(e_0) = c_1^2\\sigma_e^2\\\\ \\text{Var}(Y_1) = c_2^2\\text{Var}(Y_0) + \\text{Var}(e_0) = c_2^2c_1^2\\sigma_e^2=\\sigma_e^2(1+c_1^2c_2^2) \\end{cases} \\implies \\] \\[ \\begin{cases} c_1^2\\sigma_e^2 = \\sigma_e^2(1+c_1^2c_2^2) &amp; \\iff c_1^2(1-c_2^2) = 1\\\\ c_1^2 = \\frac{1}{1-c_2^2} &amp; \\iff c_1 = \\sqrt{\\frac{1}{1-c_1^2}} = \\frac{1}{\\sqrt{1-c_1^2}} \\end{cases} \\] with covariance \\[ \\begin{gathered} \\text{Cov}(Y_0, Y_1) = \\text{Cov}(c_1e_0,c_2 c_1 e_0 + e_1) = \\text{Cov}(c_1e_0,c_2 c_1 e_0) + \\text{Cov}(c_1e_0, e_1) =\\\\ c_1^2c_2\\sigma_e^2 + c_1\\text{Cov}(e_0, e_1) = c_1^2c_2\\sigma_e^2 + 0 \\end{gathered} \\] and autocorrelation \\[ \\rho_1 = \\frac{c_1^2c_2\\sigma_e^2}{\\sqrt{(c_1^2)^2}} = c_2 \\] so we must choose \\[ \\begin{cases} c_2 = \\frac{\\phi_1}{1-\\phi_2} \\\\ c_1 = \\frac{1}{\\sqrt{1-c_2^2}} \\end{cases}. \\] c The process can be transformed by scaling and standardizing it and then shifting with any given mean. \\[ \\frac{Y_t \\sqrt{y_0}}{c_1} + \\mu \\] 4.25 Final exercise a \\[ \\begin{split} Y_t &amp; = \\phi Y_{t-1} + e_t\\\\ &amp; = \\phi(\\phi Y_{t-2}+e_{t-1}) + e_t = \\phi^2 Y_{t-2} + \\phi e_{t-1} + e_t\\\\ &amp; \\vdots \\\\ &amp; = \\phi^t Y_{t-t} + \\phi e_{t-1} + \\phi^2 e_{t-2} + \\dots + \\phi^{t-1}e_1+e_t &amp; \\square \\end{split} \\] b \\[ E(Y_t) = E(\\phi^t Y_0 +\\phi e_{t-1} + \\phi^2 e_{t-2} + \\dots + \\phi^{t-1}e_1+e_t) =\\phi^t\\mu_0 \\] c \\[ \\begin{split} \\text{Var}(Y_t) &amp; = \\text{Var}(\\phi^t Y_0 + \\phi e_{t-1}+\\phi^2e_{t-2} + \\dots + \\phi^{t-1}e_1)\\\\ &amp; = \\phi^{2t}\\sigma_0^2+\\sigma_e^2 \\sum_{k=0}^{t-1}(\\phi^2)^k\\\\ &amp; = \\sigma_e^2 \\frac{1-\\phi^{2n}}{1-\\phi^2} + \\phi^{2t}\\sigma_0^2 \\quad\\text{if $\\phi \\neq 1$ else}\\\\ &amp; = \\text{Var}(Y_0) + \\sigma_e^2 t = \\sigma_0^2 + \\sigma_e^2 t \\end{split} \\] d If \\(\\mu_0 = 0\\) then \\(E(Y_t) = 0\\) but for \\(\\text{Var}(Y_t)\\) to be free of t, \\(\\phi\\) cannot be 1. e \\[ \\text{Var}(Y_t) = \\phi^2 \\text{Var}(Y_{t-1}) + \\sigma_e^2 \\implies \\phi^2 \\text{Var}(Y_t) + \\sigma_e^2 \\] and \\[ \\text{Var}(Y_{t-1}) = \\text{Var}(Y_t)(1-\\phi^2) = \\sigma_e^2 \\implies \\text{Var}(Y_t) = \\frac{\\sigma_e^2}{1-\\phi} \\] and then we must have \\(|\\phi| &lt; 1\\). "],
["models-for-nonstationary-time-series.html", "5 Models for Nonstationary Time Series 5.1 5.1 5.2 5.2 5.3 5.3 5.4 5.4 5.5 5.5 5.6 5.6 5.7 5.7 5.8 5.8 5.9 5.9 5.10 5.10 5.11 Winnebago c 5.12 Standard &amp; Poor 5.13 Air passengers 5.14 Rainfall", " 5 Models for Nonstationary Time Series 5.1 5.1 5.2 5.2 5.3 5.3 5.4 5.4 5.5 5.5 5.6 5.6 5.7 5.7 5.8 5.8 5.9 5.9 5.10 5.10 5.11 Winnebago a The plot in 5.1 has a trend that seems almost exponential, as well as a seasonal pattern by which sales seem to slump later in the year and surge in the spring months. data(winnebago) winnebago &lt;- as.xts(winnebago) xyplot(winnebago, ylab = &quot;Sales&quot;) Figure 5.1: Monthly unit sales of recreational vehicles. b We take the log of sales and it looks like we have made the trend linear in time 5.2 winnebago_log &lt;- log(winnebago) xyplot(winnebago_log, ylab = expression(log(sales))) Figure 5.2: Logged monthly sales. c We produce the result in figure 5.3. The patterns are similar but it seems like the fractional relative changes are greater in magnitude for the larger values of sales winnebago_frac &lt;- diff(winnebago) / lag(winnebago, 1) winnebago_logdiff &lt;- diff(log(winnebago)) winnebago_comp &lt;- merge.xts(frac = winnebago_frac, logdiff = winnebago_logdiff) xyplot(winnebago_comp, screens = c(1, 1), auto.key = TRUE, ylab = &quot;Sales&quot;, col = c(&quot;darkorange2&quot;, &quot;navy&quot;)) Figure 5.3: Comparison between differences of logs and fractional relative changes. 5.12 Standard &amp; Poor a There is an exponential trend in the time series (Figure 5.4) that, however, seems to perhaps level off after 1970 or so. data(SP) SP &lt;- as.xts(SP) xyplot(SP, grid = TRUE) Figure 5.4: Quarterly values of the Standard and Poor index. b In Figure 5.5 we’ve transformed the time series of the S&amp;P index by taking the log. The series is “more” linear but there is still an exponential pattern. sp_log &lt;- log(SP) xyplot(sp_log, ylab = expression(log(value)), grid = TRUE) Figure 5.5: Logged values of the Standard and Poor’s index. c Next, we compute the fractional relative changes and the differences of natural logartihms (Figure 5.6). We see that there is little difference between the series and only really so for the higher numbers of sales. SP_frac &lt;- diff(SP) / lag(SP, 1) SP_logdiff &lt;- diff(log(SP)) SP_comp &lt;- merge.xts(frac = SP_frac, logdiff = SP_logdiff) xyplot(SP_comp, screens = c(1, 1), auto.key = TRUE, ylab = &quot;Sales&quot;, col = c(&quot;darkorange2&quot;, &quot;navy&quot;)) Figure 5.6: Differences in logs and fractional relative differences. 5.13 Air passengers a We plot the monthly intervational airline passenger counts in Figure5.7 and note that we have a strong seasonal trend and perhaps a slight exponential increase. Variance seems to increase as well. data(airpass) airpass &lt;- as.xts(airpass) xyplot(airpass, ylab = &quot;Passengers&quot;) Figure 5.7: Monthly airline passenger totals. b As in previous exercises, we take the natural log of the dependent variable, in this case passenger totals, and note that we have made the trend linear (or maybe exponentially decreasing?) but notably have stabilized the variance of the series. airpass_log &lt;- log(airpass) xyplot(airpass_log, ylab = expression(log(airpass))) Figure 5.8: Logged monthly airline passenger totals. c Next, we compute the fractional relative changes and the differences of natural logartihms (Figure 5.6). We see that there is little difference between the series and only really so for the higher numbers of sales. airpass_frac &lt;- diff(airpass) / lag(airpass, 1) airpass_logdiff &lt;- diff(log(airpass)) airpass_comp &lt;- merge.xts(frac = airpass_frac, logdiff = airpass_logdiff) xyplot(airpass_comp, screens = c(1, 1), auto.key = TRUE, ylab = &quot;Sales&quot;, col = c(&quot;darkorange2&quot;, &quot;navy&quot;)) Figure 5.9: Differences in logs and fractional relative differences. 5.14 Rainfall a In this exercise we consider annual rainfall data for Los Angeles. We use TSA::BoxCox.ar() to train a power model to the time series (Figure 5.10), optimizing via lok-likelihood maximization. data(larain) larain &lt;- as.xts(larain) obj &lt;- BoxCox.ar(larain) Figure 5.10: Box-Cox training on the annual rainfall data. lambda &lt;- obj$lambda[which.max(obj$loglike)] The best value of \\(\\lambda\\) is 0.2. b We apply the power transformation and see the results of Q-Q plots for both the untransformed and the transformed time series in Figure 5.11 larain_trans &lt;- larain ^ lambda xyplot(list(Untransformed = larain, Transformed = larain_trans), FUN = qqmath, y.same = FALSE) Figure 5.11: Q-Q plots of untransformed and transformed time series data. d Next we plot \\(Y_t\\) versus \\(Y_{t-1}\\) in Figure 5.12 – no correlation is evident from this picture, nor would we expect that our transformatin would induce any given that it is not at all based on the previous values. xyplot(zlag(larain_trans) ~ larain_trans, ylab = &quot;Lag 1&quot;, xlab = &quot;Lag 0&quot;) Figure 5.12: Lag 0 against lag 1. "]
]
