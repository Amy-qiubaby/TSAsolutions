[
["index.html", "Solutions to Time Series Analysis with Applications in R Preface", " Solutions to Time Series Analysis with Applications in R Johan Larsson 2017-03-27 Preface This book contains solutions to the problems in the book Time Series Analysis with Applications in R, third edition, by Cryer and Chan. It is provided as a github repository so that anybody may contribute to its development. Unlike the book, the solutions here use lattice graphics when possible instead of base graphics. "],
["introduction.html", "Chapter 1 Introduction 1.1 Larain 1.2 Colors 1.3 Random, normal time series 1.4 Random, \\(\\chi^2\\)-distributed time series 1.5 t(5)-distributed, random values 1.6 Dubuque temperature series", " Chapter 1 Introduction 1.1 Larain Use software to produce the time series plot shown in Exhibit 1.2, on page 2. The data are in the file named larain. library(TSA) library(latticeExtra) data(larain, package = &quot;TSA&quot;) xyplot(larain, ylab = &quot;Inches&quot;, xlab = &quot;Year&quot;, type = &quot;o&quot;) 1.2 Colors Produce the time series plot displayed in Exhibit 1.3, on page 3. The data file is named color. data(color) xyplot(color, ylab = &quot;Color property&quot;, xlab = &quot;Batch&quot;, type = &quot;o&quot;) 1.3 Random, normal time series Simulate a completely random process of length 48 with independent, normal values. Plot the time series plot. Does it look “random”? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rnorm(48))) xyplot(as.ts(rnorm(48))) As far as we can tell there is no discernable pattern here. 1.4 Random, \\(\\chi^2\\)-distributed time series Simulate a completely random process of length 48 with independent, chi-square distributed values, each with 2 degrees of freedom. Display the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rchisq(48, 2))) xyplot(as.ts(rchisq(48, 2))) The process appears random, though non-normal. 1.5 t(5)-distributed, random values Simulate a completely random process of length 48 with independent, t-distributed values each with 5 degrees of freedom. Construct the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time. xyplot(as.ts(rt(48, 5))) xyplot(as.ts(rt(48, 5))) It looks random but not normal, though it should be approximately so, considering the distribution that we have sampled from. 1.6 Dubuque temperature series Construct a time series plot with monthly plotting symbols for the Dubuque temperature series as in Exhibit 1.7, on page 6. The data are in the file named tempdub. data(tempdub) xyplot(tempdub, ylab = &quot;Temperature&quot;, xlab = &quot;Year&quot;) "],
["fundamental-concepts.html", "Chapter 2 Fundamental concepts 2.1 Expected value and covariance 2.2 Covariance and dependence 2.3 Weak stationarity, autocovariance and time plot 2.4 2.5 2.6 2.7 2.8 2.9 2.10 2.11 2.12 2.13 ", " Chapter 2 Fundamental concepts 2.1 Expected value and covariance Suppose \\(\\text{E}[X) = 2, \\text{Var}[X) = 9, \\text{E}[Y) = 0, \\text{Var}[Y) = 4\\), and \\(\\text{Corr}[X,Y) = 0.25\\). Find: \\(\\text{Var}(X + Y)\\). \\(\\text{Cov}(X, X + Y)\\). \\(\\text{Corr}(X + Y, X − Y)\\). \\begin{align} \\text{Cov}[X,Y] &amp; = \\text{Corr}[X,Y]\\sqrt{Var[X]Var[Y]}\\\\ &amp; = 0.25 \\sqrt{9 \\times 4} = 1.5 \\\\ \\text{Var}[X,Y] &amp; = Var[X]+Var[Y]+2Cov[X,Y]\\\\ &amp; = 9 + 4 + 2 \\times 3 = 16\\\\ \\end{align} \\[\\text{Cov}[X, X+Y] = \\text{Cov}[X,X] + \\text{Cov}[X,Y] = \\text{Var}[X] + \\text{Cov}[X,Y] = 9 + 1.5 = 10.5\\] \\begin{align} \\text{Corr}[X+Y, X-Y] = &amp; \\text{Corr}[X,X] + \\text{Corr}[X,-Y] + \\text{Corr}[Y,X] + \\text{Corr}[Y,-Y] \\\\ = &amp; \\text{Corr}[Y,X] + \\text{Corr}[Y,-Y] \\\\ = &amp; 1 - 0.25 + 0.25 -1 \\\\ = &amp; 0 \\\\ \\end{align} 2.2 Covariance and dependence If \\(X\\) and \\(Y\\) are dependent but \\(\\text{Var}[X] = \\text{Var}[Y]\\), find \\(\\text{Cov}[X + Y, X − Y]\\). \\[ \\text{Cov}[X+Y,X-Y] = \\text{Cov}[X,X] + \\text{Cov}[X,-Y] + \\text{Cov}[Y,X] + \\text{Cov}[Y, -Y] = \\\\ Var[X] - Cov[X,Y] + Cov[X,Y] - Var[Y] = 0 \\] since \\(Var[X] = Var[Y]\\). 2.3 Weak stationarity, autocovariance and time plot Let X have a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and let \\(Y_t = X\\) for all \\(t\\). Show that \\(\\{Yt\\}\\) is strictly and weakly stationary. Find the autocovariance function for \\(\\{Yt\\}\\). Sketch a “typical” time plot of Yt. We have that \\[ P(Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n}) =\\\\ P(X_1, X_2, \\dots, X_n) =\\\\ P(Y_{t_1 - k}, Y_{t_2 - k}, \\dots, Y_{t_n - k}), \\] which satisfies our requirement for strict stationarity. The autocovariance is given by \\[ \\gamma_{t,s}=\\text{Cov}[Y_t, Y_s] = \\text{Cov}[X,X] = \\text{Var}[X] = \\sigma^2. \\] library(lattice) tstest &lt;- ts(runif(100)) lattice::xyplot(tstest, panel = function(x, y, ...) { panel.abline(h = mean(y), lty = 2) panel.xyplot(x, y, ...) }) Figure 2.1: A white noise time series: no drift, independence between observations. 2.4 Let \\(\\{e_t\\}\\) be a zero mean white noise process. Suppose that the observed process is \\(Y_t = e_t + \\theta e_t − 1\\), where \\(\\theta\\) is either 3 or 1/3. Find the autocorrelation function for \\(\\{Yt\\}\\) both when \\(\\theta = 3\\) and when \\(\\theta = 1/3\\). You should have discovered that the time series is stationary regardless of the value of \\(\\theta\\) and that the autocorrelation functions are the same for \\(\\theta = 3\\) and \\(\\theta = 1/3\\). For simplicity, suppose that the process mean is known to be zero and the variance of \\(Y_t\\) is known to be 1. You observe the series \\(\\{Yt\\}\\) for \\(t = 1, 2, \\dots , n\\) and suppose that you can produce good estimates of the autocorrelations \\(\\rho_k\\). Do you think that you could determine which value of \\(\\theta\\) is correct (3 or 1/3) based on the estimate of \\(\\rho_k\\)? Why or why not? \\[ E[Y_t] = E[e_t+\\theta e_{t-1}] = E[e_t] + \\theta E[e_{t-1}] = 0 + 0 = 0\\\\ V[Y_t] = V[e_t + \\theta e_{t-1}] = V[e_t] + \\theta^2 V[e_{t-1}] = \\sigma_e^2 + \\theta^2 \\sigma_e^2 = \\sigma_2^2(1 + \\theta^2)\\\\ \\] For \\(k = 1\\) we have \\[ C[e_t + \\theta e_{t-1}, e_{t-1} + \\theta e_{t-2}] = \\\\ C[e_t,e_{t-1}] + C[e_t, \\theta e_{t-2}] + C[\\theta e_{t-1}, e_{t-1}] + C[\\theta e_{t-1}, \\theta e_{t-2}] = \\\\ 0 + 0 + \\theta V[e_{t-1}] + 0 = \\theta \\sigma_e^2,\\\\ \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\theta \\sigma_e^2}{\\sqrt{(\\sigma_e^2(1+\\theta^2))^2}} = \\frac{\\theta }{1+\\theta^2} \\] and for \\(k = 0\\) we get \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\text{Corr}[Y_t, Y_t] = 1 \\] and, finally, for \\(k &gt; 0\\): \\[ C[e_t + \\theta e_{t-1}, e_{t-k} + \\theta e_{t-k-1}] = \\\\ C[e_t, e_{t-k}] + C[e_t, e_{t-1-k}] + C[\\theta e_{t-1}, e_{t-k}] + C[\\theta e_{t-1}, \\theta e_{t-1-k}] = 0 \\] given that all terms are independent. Taken together, we have that \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\begin{cases} 1 &amp; \\quad \\text{for } k = 0\\\\ \\frac{\\theta}{1 + \\theta^2} &amp; \\quad \\text{for } k = 1\\\\ 0 &amp; \\quad \\text{for } k &gt; 1 \\end{cases}. \\] And, as required, \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\begin{cases} \\frac{3}{1+3^2} = \\frac{3}{10} &amp; \\quad \\text{if } \\theta = 3\\\\ \\frac{1/3}{1 + (1/3)^2} = \\frac{1}{10/3} = \\frac{3}{10} &amp; \\quad \\text{if } \\theta = 1/3 \\end{cases}. \\] No, probably not. Given that \\(\\rho\\) is standardized, we will not be able to detect any difference in the variance regardless of the values of k. 2.5 Suppose \\(Y_t = 5 + 2t + X_t\\), where \\(\\{X_t\\}\\) is a zero-mean stationary series with autocovariance function \\(\\gamma_k\\). Find the mean function for \\(\\{Y_t\\}\\). Find the autocovariance function for \\(\\{Y_t\\}\\). Is \\(\\{Y_t\\}\\) stationary? Why or why not? \\[\\mu_t = E[Y_t] = E[5 + 2t + X_t] = 5 + 2E[t] + E[X_t] = 5 + 2t + 0 = 2t + 5\\] \\[ \\gamma_k = \\text{Corr}[5+2t+X_t, 5+2(t-k)+X_{t-k}] = \\text{Corr}[X_t, X_{t-k}]\\] No, the mean function (\\(\\mu_t\\)) is constant and the aurocovariance (\\(\\gamma_{t,t-k}\\)) free from \\(t\\). 2.6 Let {Xt} be a stationary time series, and define \\[ Y_t = \\begin{cases} X_t &amp; \\quad \\text{for odd } t \\\\ X_t + 3 &amp; \\quad \\text{for even } t \\end{cases}. \\] Show that \\(\\text{Cov}[Y_t, Y_{t-k}]\\) is free from \\(t\\) for all lags \\(k\\). iS \\(\\{Y_t\\}\\) stationary? \\[\\text{Cov}[a + X_t, b + X_{t-k}] =\\text{Cov}[X_t, X_{t-k}],\\] which is free from \\(t\\) for all \\(k\\) because \\(X_t\\) is stationary. \\[ \\mu_t = E[Y_t] = \\begin{cases} E[X_t] &amp; \\quad \\text{for odd } t\\\\ 3 + E[X_t] &amp; \\quad \\text{for even } t\\\\ \\end{cases}. \\] Since \\(\\mu_t\\) varies depending on \\(t\\), \\(Y_t\\) is not stationary. 2.7 Suppose that \\(\\{Y_t\\}\\) is stationary with autocovariance function \\(\\gamma_k\\). Show that \\(W_t = \\triangledown Y_t = Y_t − Y_t − 1\\) is stationary by finding the mean and autocovariance function for \\(\\{W_t\\}\\). Show that $U_t = 2Y_t = = Y_t − 2Y_t − 1 + Y_t − 2 is stationary. (You need not find the mean and autocovariance function for \\(\\{U_t\\}\\).) \\[\\mu_t = E[W_t] = E[Y_t - Y_{t-1}] = E[Y_t] - E[Y_{t-1}] = 0\\] because \\(Y_t\\) is stationary. \\[ \\text{Cov}[W_t] = \\text{Cov}[Y_t - Y_{t-1}, Y_{t-k} - Y_{t-1-k}] = \\\\ \\text{Cov}[Y_t, Y_{t-k}] + \\text{Cov}[Y_t, Y_{t-1-k}] + \\text{Cov}[-Y_{t-k}, Y_{t-k}] + \\text{Cov}[-Y_{t-k}, -Y_{t-1-k}]=\\\\ \\gamma_k-\\gamma_{k+1}-\\gamma_{k-1}+\\gamma_{k} = 2 \\gamma_k - \\gamma_{k+1} - \\gamma_{k-1}. \\quad \\square \\] In (a), we discovered that the difference between two stationary processes, \\(\\triangledown Y_t\\) itself was stationary. It follows that the difference between two of these differences, \\(\\triangledown^2Y_t\\) is also stationary. 2.8 Suppose that \\(\\{Y_t\\}\\) is stationary with autocovariance function \\(\\gamma_k\\). Show that for any fixed positive integer \\(n\\) and any constants \\(c_1, c_2, \\dots, c_n\\), the process \\(\\{W_t\\}\\) defined by \\(W_t = c_1 Y_t + c_2 Y_{t–1} + \\dots + c_n Y_{t-n+1}\\) is stationary. (Note that Exercise 2.7 is a special case of this result.) \\begin{align} E[W_t] &amp; = c_1E[Y_t]+c_2E[Y_t] + \\dots + c_n E[Y_t]\\\\ &amp; = E[Y_t](c_1 + c_2 + \\dots + c_n), \\end{align} and thus the expected value is constant. Moreover, \\begin{align} \\text{Cov}[W_t] &amp; = \\text{Cov}[c_1 Y_t + c_2 Y_{t-1} + \\dots + c_n Y_{t-k}, c_1 Y_{t-k} + c_2 Y_{t-k-1} + \\dots + c_n Y_{t-k-n}] \\\\ &amp; = \\sum_{i=0}^n \\sum_{j=0}^n c_i c_j \\text{Cov}[Y_{t-j}Y_{t-i-k}] \\\\ &amp; = \\sum_{i=0}^n \\sum_{j=0}^n c_i c_j \\gamma_{j-k-i}, \\end{align} which is free of \\(t\\); consequently, \\(W_t\\) is stationary. 2.9 Suppose _t = _0 + _1 t + X_t$, where \\(\\{X_t\\}\\) is a zero-mean stationary series with autocovariance function \\(\\gamma_k\\) and \\(\\beta_0\\) and \\(\\beta_1\\) are constants. Show that \\(\\{Y_t\\}\\) is not stationary but that \\(W_t = \\triangledown Y_t = Y_t − Y_{t−1}\\) is stationary. In general, show that if \\(Y_t = \\mu_t + X_t\\), where \\(\\{X_t\\}\\) is a zero-mean stationary series and \\(\\mu_t\\) is a polynomial in \\(t\\) of degree \\(d\\), then \\(\\triangledown^m Y_t = \\triangledown(\\triangledown^{m−1}Y_t)\\) is stationary for \\(m \\geq d\\) and nonstationary for \\(0 \\leq m &lt; d\\). \\[ E[Y_t] = \\beta_0 + \\beta_1 t + E[X_t] = \\beta_0 + \\beta_1 t + \\mu_{t_x}, \\] which is not free of \\(t\\) and hence not stationary. \\[ \\text{Cov}[Y_t] = \\text{Cov}[X_t, X_t-1] = \\gamma_{t-1} \\] \\[ E[W_t] = E[Y_t - Y_{t-1}] = E[\\beta_0 + \\beta_1 t + X_t - (\\beta_0 + \\beta_1(t-1) + X_{t-1})] =\\\\ \\beta_0 + \\beta_1 t - \\beta_0 - \\beta_1 t + \\beta_1 = \\beta_1, \\] is free of \\(t\\) and, furthermore, we have \\[ \\text{Cov}[W_t] = \\text{Cov}[\\beta_0 + \\beta_1 t + X_t, \\beta_0 + \\beta_1 (t-1) + X_{t-1}] =\\\\ \\text{Cov}[X_t, X_{t-1}] = \\gamma_k, \\] which is also free of \\(t\\), thereby proving that \\(W_t\\) is stationary. \\[ E[Y_t] = E[\\mu_t + X_t] = \\mu_t + \\mu_t = 0 + 0 = 0, \\quad \\text{and}\\\\ \\text{Cov}[Y_t] = \\text{Cov}[\\mu_t + X_t, \\mu_{t-k} + X_{t-k}] = \\text{Cov}[X_t, X_{t-k}] = \\gamma_k \\] \\[ \\triangledown^m Y_t = \\triangledown(\\triangledown^{m−1}Y_t) \\] Currently unsolved. 2.10 Let \\(\\{X_t\\}\\) be a zero-mean, unit-variance stationary process with autocorrelation function \\(\\rho_k\\). Suppose that \\(\\mu_t\\) is a nonconstant function and that \\(\\sigma_t\\) is a positive-valued nonconstant function. The observed series is formed as \\(Y_t = \\mu_t + \\sigma_t X_t\\). Find the mean and covariance function for the \\(\\{Y_t\\}\\) process. Show that the autocorrelation function for the \\(\\{Y_t\\}\\) process depends only on the time lag. Is the \\(\\{Y_t\\}\\) process stationary? Is it possible to have a time series with a constant mean and with \\(\\text{Corr}(Y_t ,Y_t − k)\\) free of \\(t\\) but with \\(\\{Y_t\\}\\) not stationary? \\[ \\mu_t = E[Y_t] = E[\\mu_t + \\sigma_t X_t] = \\mu_t + \\sigma_t E[X_t] = \\mu_t + \\sigma_t \\times 0 = \\mu_t\\\\ \\gamma_{t,t-k} = \\text{Cov}[Y_t] = \\text{Cov}[\\mu_t + \\sigma_t X_t, \\mu_{t-k} + \\sigma_{t-k} X_{t-k}] = \\sigma_t \\sigma_{t-k} \\text{Cov}[X_t, X_{t-k}] = \\sigma_t \\sigma_{t-k} \\rho_k \\] First, we have \\[ \\text{Var}[Y_t] = \\text{Var}[\\mu_t + \\sigma_t X_t] = 0 + \\sigma_t^2 \\text{Var}[X_t] = \\sigma_t^2 \\times 1 = \\sigma_t^2 \\] since \\(\\{X_t\\}\\) has unit-variance. Futhermore, \\[ \\text{Corr}[Y_t, Y_{t-k}] = \\frac{\\sigma_t \\sigma_{t-k} \\rho_k}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t-k}]}} = \\frac{\\sigma_t \\sigma_{t-k}\\rho_k}{\\sigma_t \\sigma_{t-k}} = \\rho_k, \\] which depends only on the time lag, \\(k\\). However, \\(\\{Y_t\\}\\) is not necessarily stationary since \\(\\mu_t\\) may depend on \\(t\\). Yes, \\(\\rho_k\\) might be free from \\(t\\) but if \\(\\sigma_t\\) is not, we will have a non-stationary time series with autocorrelation free from \\(t\\) and constant mean. 2.11 Suppose \\(\\text{Cov}(X_t,X_t − k) = \\gamma_k\\) is free of \\(t\\) but that \\(E(X_t) = 3t\\). Is \\(\\{X_t\\}\\) stationary? Let \\(Y_t = 7 − 3t + X_t\\). Is \\(\\{Y_t\\}\\) stationary? \\[ \\text{Cov}[X_t, X_{t-k}] = \\gamma_k\\\\ E[X_t] = 3t \\] \\(\\{X_t\\}\\) is not stationary because \\(\\mu_t\\) varies with \\(t\\). \\[ E[Y_t] = 3 - 3t+E[X_t] = 7 - 3t - 3t = 7\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[7-3t+X_t,7-3(t-k)+X_{t-k}] = \\text{Cov}[X_t, X_{t-k}] = \\gamma_k \\] Since the mean function of \\(\\{Y_t\\}\\) is constant (7) and its autocovariance free of \\(t\\), \\(\\{Y_t\\}\\) is stionary. 2.12 Suppose that $Y_t = e_t − e_{t−12}. Show that \\(\\{Y_t\\}\\) is stationary and that, for \\(k &gt; 0\\), its autocorrelation function is nonzero only for lag \\(k = 12\\). \\[ E[Y_t] = E[e_t - e_{t-12}] = E[e_t] - E[e_{t-12}] = 0\\\\ \\text{Cov}[Y_t, Y_{t-k}] = \\text{Cov}[e_t - e_{t-12}, e_{t-k} - e_{t-12-k}] =\\\\ \\text{Cov}[e_t, e_{t-k}] - \\text{Cov}[e_t, e_{t-12-k}] - \\text{Cov}[e_{t-12}, e_{t-k}] + \\text{Cov}[e_{t-12}, e_{t-12-k}] \\] Then, as required, we have \\[ \\text{Cov}[Y_t, Y_{t-k}] = \\begin{cases} \\text{Cov}[e_t, e_{t-12}] - \\text{Cov}[e_t, e_t] - \\text{Cov}[e_{t-12}, e_{t-12}] + \\text{Cov}[e_{t-12},e_t] = \\text{Var}[e_t] - \\text{Var}[e_{t-12}] \\neq 0 &amp; \\quad \\text{for } k=12\\\\ 0 + 0 + 0 + 0 = 0 &amp; \\quad \\text{for } k \\neq 12\\quad \\square \\end{cases} \\] 2.13 "]
]
